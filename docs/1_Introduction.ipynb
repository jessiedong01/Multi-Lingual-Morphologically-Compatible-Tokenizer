{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbeddf0-84f3-4a8a-82f9-f2fc2dd02219",
   "metadata": {},
   "source": [
    "# Exploring Tokenizers üßê\n",
    "*(contact: arjo@stanford.edu)*\n",
    "\n",
    "This notebook walks through what a tokenizer is and the state-of-the-art tokenizers currently used. Including where they might struggle!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c54da5-0fa6-41c7-a9b0-ea125a9f1cb9",
   "metadata": {},
   "source": [
    "## 1. What is a tokenizer? ü§î\n",
    "Tokenization is the fundamental first step in nearly all Natural Language Processing (NLP) tasks. It's the process of breaking down a continuous stream of text into smaller, meaningful units called tokens. Think of it as digitally dicing a sentence into its core ingredients. These tokens could be words, characters, or parts of words (subwords).\n",
    "\n",
    "For example, the sentence:\n",
    "```\"NLP is fascinating!\"```\n",
    "\n",
    "Might be tokenized into:\n",
    "```[\"NLP\", \"is\", \"fascinating\", \"!\"]```\n",
    "\n",
    "These tokens are then converted into numerical representations that machine learning models, like Large Language Models (LLMs), can understand and process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d3aef-8ddf-4bec-bc27-46e35c35c5fe",
   "metadata": {},
   "source": [
    "### 1.1 A Quick Primer on UTF-8 üî°\n",
    "To understand how modern tokenizers work (and where they fail), it's essential to know how text is represented digitally. Computers don't see characters like 'A', '√©', or 'üòÇ'; they see bytes.\n",
    "\n",
    "**Unicode** is the universal standard that assigns a unique number, called a **code point**, to every character in every language. For example, the code point for the emoji 'üòÇ' is `U+1F602`.\n",
    "\n",
    "**UTF-8** is the dominant **encoding** that translates these Unicode code points into a sequence of bytes. Its most important feature is that it's a **variable-width encoding**. This means different characters take up a different number of bytes:\n",
    "* **1 byte:** For all standard English characters and symbols, i.e. ASCII (`A-Z`, `0-9`, `!`, etc.).\n",
    "* **2 bytes:** For many accented letters and symbols from other alphabets (e.g., `√©`, `√±`).\n",
    "* **3 bytes:** For most common Chinese, Japanese, and Korean (CJK) characters (e.g., `Êó•`, `Êú¨`).\n",
    "* **4 bytes:** For emojis and less common characters (e.g., `üòÇ`, `üß†`).\n",
    "\n",
    "The crucial takeaway is that a single character a human sees can be composed of multiple bytes. This fact has profound implications for tokenization algorithms that operate at the byte level (i.e. multi-byte symbols could be chopped into multiple tokens!).\n",
    "This is particularly an issue for non-english languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e848830-2956-4539-befc-0c45b567731a",
   "metadata": {},
   "source": [
    "#### A Hands-On Look at UTF-8 Bytes\n",
    "\n",
    "As we discussed, UTF-8 is a variable-width encoding. Let's see what that actually means by encoding a few characters and looking at their raw bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a090cbc-906d-410b-9d9a-934dcd5716fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A standard ASCII character (1 byte)\n",
    "char_1 = 'H'\n",
    "bytes_1 = char_1.encode('utf-8')\n",
    "print(f\"'{char_1}' -> {bytes_1}  (Length: {len(bytes_1)} byte)\")\n",
    "\n",
    "# An accented Latin character (2 bytes)\n",
    "char_2 = '√©'\n",
    "bytes_2 = char_2.encode('utf-8')\n",
    "print(f\"'{char_2}' -> {bytes_2}  (Length: {len(bytes_2)} bytes)\")\n",
    "\n",
    "# A Japanese character (3 bytes)\n",
    "char_3 = 'Âèã'\n",
    "bytes_3 = char_3.encode('utf-8')\n",
    "print(f\"'{char_3}' -> {bytes_3}  (Length: {len(bytes_3)} bytes)\")\n",
    "\n",
    "# An emoji (4 bytes)\n",
    "char_4 = 'üòä'\n",
    "bytes_4 = char_4.encode('utf-8')\n",
    "print(f\"'{char_4}' -> {bytes_4}  (Length: {len(bytes_4)} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e87f7e-fe6e-4f92-9afb-0a9b428642b1",
   "metadata": {},
   "source": [
    "As you can see, the number of bytes needed to represent a single character varies. Now, let's see what happens when we combine two characters. The bytes are simply concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d05d5e3-bb48-4cc2-9688-c9b0dce3e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hüòä\"\n",
    "text_bytes = text.encode('utf-8')\n",
    "print(f\"'{text}' -> {text_bytes}\")\n",
    "print(f\"Individual bytes: {bytes_1} + {bytes_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b115c32-3a93-4ed4-981b-e38d7d2428c9",
   "metadata": {},
   "source": [
    "This is the key insight: to the computer, `\"Hüòä\"` is just the sequence of five bytes `b'H\\xf0\\x9f\\x98\\x8a'`. A simple byte-level algorithm has no inherent knowledge that the last four bytes represent a single smiley face.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c1b34-9c6d-47f8-9282-4b811f28d905",
   "metadata": {},
   "source": [
    "### 1.2 Modern Tokenization Techniques\n",
    "While early methods simply split text by spaces and punctuation, this approach is too rigid. It can't handle punctuation within words (like `O'Malley`), hyphenated terms (`state-of-the-art`), or languages without clear word boundaries (like Japanese or Chinese).\n",
    "\n",
    "Today, the most successful and widely used techniques are based on subword tokenization. These methods break words into smaller, frequently occurring pieces. This approach cleverly balances the need for a manageable vocabulary size with the ability to represent any word, including rare, misspelled, or new ones.\n",
    "\n",
    "The dominant subword algorithms include:\n",
    "* **[Byte-Pair Encoding (BPE)](https://arxiv.org/abs/1508.07909):** This is a data-driven algorithm that starts with a vocabulary of individual characters. It iteratively counts the most frequent pair of adjacent tokens and merges them into a new, single token. This process is repeated for a set number of merges, resulting in a vocabulary of common subwords. For example, it might learn to merge `e` and `r` into `er`, then `er` and `s` into `ers`.\n",
    "    * **Where it fails:** Because BPE often operates at the **byte level**, it has no inherent understanding of character boundaries. An emoji like `üòÇ` is made of four bytes. If the last byte of `üòÇ` and the first byte of the next character happen to be a frequent pair in the training data, BPE will happily merge them. This creates nonsensical \"Frankenstein tokens\" that **split a single character across two different tokens**, destroying its meaning. This is a common problem for emojis, CJK characters, and accented letters, leading to a less efficient and less meaningful vocabulary.\n",
    "* **[WordPiece](https://huggingface.co/learn/llm-course/en/chapter6/5):** Used by Google's BERT model, WordPiece is very similar to BPE. However, instead of merging the most frequent pair, it merges the pair that maximizes the likelihood of the training data if it were added to the vocabulary. It essentially asks, \"Which merge gives us the most bang for our buck in terms of explaining the text?\"\n",
    "* **[Unigram Language Model](https://huggingface.co/learn/llm-course/en/chapter6/7):** This approach, used by models like T5 and ALBERT, takes a different route. It starts with a very large set of possible subwords and iteratively removes the ones that contribute least to the overall probability of the corpus, gradually shrinking the vocabulary to the desired size. A key feature is that it's probabilistic, meaning a single word can have multiple valid tokenizations, adding flexibility.\n",
    "\n",
    "BPE seems to be the favorite of many recent systems, e.g. [DeepSeek-v3](https://arxiv.org/pdf/2412.19437) uses a modified BPE implementation and Cohere labs [1M tokenizer](https://arxiv.org/pdf/2506.10766).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f67ca9-3634-4af3-962b-975b2e4f6842",
   "metadata": {},
   "source": [
    "### 1.3 Where Tokenization Falters\n",
    "Despite their sophistication, modern tokenizers have significant blind spots, especially when dealing with the complexity of global human language.\n",
    "#### The Multilingual Challenge\n",
    "The biggest failure point is handling multilingual text and code-switching (mixing languages in one sentence). Most large models are trained with a single, unified vocabulary. While massive, this vocabulary is inevitably biased towards the dominant language in the training data (usually English).\n",
    "\n",
    "When a tokenizer built on an English-heavy vocabulary encounters a word from another language, it often fails to find known subwords. The only option is to fall back to the most basic units: individual characters or bytes.\n",
    "\n",
    "Consider the German word `Lebensabschnittspartner` (a partner for a phase of your life).\n",
    "\n",
    "* An ideal, German-aware tokenizer might split it meaningfully: `[\"Lebens\", \"abschnitts\", \"partner\"]`.\n",
    "* A typical English-centric tokenizer, however, might produce something nonsensical like: `[\"Leb\", \"ens\", \"abschnitt\", \"sp\", \"artner\"]` or even worse, break it into individual letters if the subwords aren't in its vocabulary.\n",
    "\n",
    "This \"character-level\" degradation destroys the word's semantic meaning before the model even sees it, making it incredibly difficult for the model to understand the text's intent.\n",
    "\n",
    "#### Other Key Failure Points\n",
    "* **Morphologically Rich Languages:** Languages like Turkish, Finnish, or Hungarian attach long strings of suffixes to a root word to convey meaning. Subword tokenizers can struggle to consistently identify the root word, often splitting these complex words in arbitrary ways.\n",
    "* **Domain-Specific Jargon:** A tokenizer trained on general web text will perform poorly on specialized documents, like medical research papers or legal contracts. It will break down crucial terms like **immunosuppressant** or **mandamus** into less meaningful fragments.\n",
    "* **Inconsistency:** Because the tokenization process is greedy and deterministic, a tiny change in a word can lead to a completely different tokenization, making the model brittle. For example, **(hello)** and **\\[hello\\]** might be tokenized differently in a way that loses the core meaning of **hello**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42f1b0-0590-4c5a-b98f-0c9725556bd2",
   "metadata": {},
   "source": [
    "#### BPE in Action: The `gpt2` Tokenizer and Frankenstein tokens\n",
    "Now, let's see how a classic byte-level BPE tokenizer, like the one used for `gpt2`, handles this. We'll load it directly from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffec5f2-17d3-4486-8cd5-fca5af400d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below for the HuggingFace transformer library. Preferably put this in an environment. e.g.\n",
    "#!conda env create myenv\n",
    "#!conda activate myenv\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15459eac-8287-4027-b968-2c173fcc3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the gpt2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Hüòä\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(f\"Original text: '{text}'\")\n",
    "print(f\"Tokenized output: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75dafd4-2b47-4b7c-a0df-640588d678d3",
   "metadata": {},
   "source": [
    "Notice the result: `['H', '√∞≈Åƒ∫', 'ƒ¨']`.\n",
    "The emoji `üòä` was not kept as a single unit. It was split into two bizarre-looking tokens: `'√∞≈Åƒ∫'` and `'ƒ¨'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b925b99-016a-4857-82af-25e867f78f43",
   "metadata": {},
   "source": [
    "#### Why Did This Happen?\n",
    "\n",
    "This is the \"Frankenstein token\" problem in action. The `gpt2` tokenizer's learned vocabulary doesn't contain a single token for `üòä`. It only knows how to represent the individual bytes that make it up. Let's prove it by inspecting the raw bytes of those strange tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2982003-e603-4e25-a794-0e58f79a74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# We need this utility to manually create the byte-to-char mapping\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"üòä\"\n",
    "\n",
    "# --- Step 1: Manually create the byte decoder ---\n",
    "# The bytes_to_unicode() function gives us the mapping from an integer (0-255)\n",
    "# to its string representation (e.g., '√∞'). We need to reverse it.\n",
    "byte_encoder = bytes_to_unicode()\n",
    "byte_decoder = {v: k for k, v in byte_encoder.items()}\n",
    "\n",
    "# --- Step 2: Get the Token IDs ---\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"Original text: '{text}'\")\n",
    "print(f\"Token IDs: {token_ids}\\n\")\n",
    "\n",
    "# --- Step 3: Create a map from IDs back to their token string representations ---\n",
    "id_to_token_string = {v: k for k, v in tokenizer.get_vocab().items()}\n",
    "\n",
    "# --- Step 4: Convert the weird token strings to their raw bytes ---\n",
    "print(\"Inspecting the bytes of each token ID:\")\n",
    "all_reconstructed_bytes = b''\n",
    "for i, token_id in enumerate(token_ids):\n",
    "    # Find the string label for the ID (e.g., '√∞≈Åƒ∫')\n",
    "    token_string = id_to_token_string[token_id]\n",
    "    \n",
    "    # Use our manually created byte_decoder to get the raw byte values (integers)\n",
    "    byte_values = [byte_decoder[char] for char in token_string]\n",
    "    \n",
    "    # Convert the integer values into a Python bytes object\n",
    "    token_as_bytes = bytes(byte_values)\n",
    "    \n",
    "    print(f\"  - ID {token_id} -> '{token_string}' -> represents bytes: {token_as_bytes}\")\n",
    "    all_reconstructed_bytes += token_as_bytes\n",
    "\n",
    "# --- Step 5: Compare with the original ---\n",
    "original_bytes = text.encode('utf-8')\n",
    "print(f\"\\nReconstructed bytes: {all_reconstructed_bytes}\")\n",
    "print(f\"Original emoji bytes:  {original_bytes}\")\n",
    "\n",
    "if all_reconstructed_bytes == original_bytes:\n",
    "    print(\"\\n‚úÖ Success! The bytes from the broken tokens perfectly match the original.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046d332-3986-4f69-87aa-036b85eb7fef",
   "metadata": {},
   "source": [
    "This corrected approach clearly demonstrates the \"Frankenstein token\" problem without causing errors:\n",
    "* The Problem: The `gpt2` tokenizer did not have a single token ID in its vocabulary for `üòä`.\n",
    "* The Fallback: Its fallback plan was to represent the emoji by its constituent UTF-8 bytes. The sequence of IDs `[240, 157, 156, 138]` is `gpt2`'s internal representation for the byte sequence `b'\\xf0\\x9f\\x98\\x8a'`.\n",
    "* The Result: Each of those IDs decodes to a strange-looking string fragment (`'√∞'`, `'≈Å'`, `'ƒ∫'`, `'ƒ¨'`). These are just human-readable labels for the raw bytes. When we see `['H', '√∞', '≈Å', 'ƒ∫', 'ƒ¨']` as the tokenization of `\"Hüòä\",` we are seeing a single character (`üòä`) being split across four different tokens.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde5156-8eb8-41cc-99c7-64180c96c2ff",
   "metadata": {},
   "source": [
    "## Q: For you to try ‚≠ê\n",
    "Find modern tokenizers on HuggingFace. Find examples of \"Other Key Failure Points\".\n",
    "* **Exemplify:** Try with a couple of hardcoded words\n",
    "* **Quantify:** Download some multilingual data (hint: see `data.py`) and quantify how well your tokenizer of choice works compared to English.\n",
    "* **Discuss:** What failure modes do you find? How do you think that might impact language understanding for computers? (e.g. LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526c987-22ce-4bd2-b32d-dfa291fe02a1",
   "metadata": {},
   "source": [
    "## 2. Beyond BPE: A Smarter, Language-Aware Tokenizer üß†\n",
    "Standard tokenizers like BPE are powerful, but they're also **language-blind**. They treat text as a stream of data to be compressed, relying purely on character frequency. This works reasonably well for a single language like English, but when faced with multilingual text, they often resort to butchering unfamiliar words into meaningless letters.\n",
    "\n",
    "This project introduces a different philosophy: a **linguistically-aware tokenizer** that acts less like a data compressor and more like a digital linguist. It learns from raw text, but its decisions are guided by pre-coded knowledge about the structure and patterns of human language. By combining data-driven statistics with linguistic rules, it learns a more robust and meaningful vocabulary, especially for multilingual corpora.\n",
    "\n",
    "It frames tokenization not as a greedy merging task, but as an **optimization problem**: What is the best possible way to segment this sentence to minimize a total \"cost\"? This cost is a sophisticated blend of statistical likelihood and linguistic plausibility.\n",
    "\n",
    "---\n",
    "\n",
    "#### What Makes It Different?\n",
    "\n",
    "This tokenizer's methodology deviates significantly from traditional approaches. It incorporates several key features that allow it to understand text at a deeper, more structural level.\n",
    "\n",
    "* **Morphological Awareness:** Instead of just seeing frequent letters, this tokenizer learns the \"shape\" of words. It understands that `run` + `-ing` is a common English pattern and `yap` + `-mak` is a common Turkish one.\n",
    "    * **Methodology:** It uses a `MorphologyEncoder` to create vector representations of words based on their character n-grams and known affixes (e.g., `-ed`, `re-`, `-ung`). This allows it to calculate a \"morphological fit\" score, rewarding tokens that look structurally correct for a given language. BPE/WordPiece have no concept of morphology.\n",
    "* **Cross-Lingual Grammatical Links:** The model can recognize that the `-s` in English \"cats\" and the `-ler` in Turkish \"kediler\" serve the same grammatical purpose (pluralization).\n",
    "    * **Methodology:** It uses a predefined map (`CROSS_EQUIV`) of grammatically equivalent suffixes across languages. This encourages the model to learn and reward tokens that exhibit these fundamental cross-lingual patterns, a feature entirely absent in standard tokenizers.\n",
    "* **Global Optimization over Greedy Merging:** Finding the best segmentation is treated as a \"shortest path problem,\" not a series of one-off greedy decisions.\n",
    "    * **Methodology:** The core of the training algorithm is **column generation**, an optimization technique. In each iteration, it decodes the entire corpus with its current vocabulary. Then, it searches for new tokens (\"columns\") that will provide the greatest overall reduction in the segmentation cost. This holistic approach avoids the irreversible, sometimes sub-optimal, merges that BPE makes.\n",
    "* **Explicit Linguistic Priors:** The tokenizer is bootstrapped with a set of explicit rules and heuristics that guide the learning process from the start.\n",
    "    * **Methodology:** It uses a flexible `LinguisticModels` system that provides direct rewards or penalties. This includes protecting atomic units like **URLs and emails** from being split, rewarding common token sequences (e.g., a capitalized word following a period), and penalizing nonsensical ones. This injects common-sense knowledge directly into the cost function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
