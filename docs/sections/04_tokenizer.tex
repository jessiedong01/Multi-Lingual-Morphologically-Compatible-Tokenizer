
%%%
%%% Tokenizer Algorithm – reflects the actual implementation
%%

Our tokenizer is a column-generation solver that alternates between (1) dynamic-programming (DP) segmentation over a pruned lattice, (2) pricing new tokens via reduced costs, and (3) updating a morphology encoder whose embeddings drive cross-lingual rewards. This section documents the concrete loss terms, iteration logic, and convergence guarantees.

% Requires \usepackage{multicol} in the main preamble

%% ─────────────────────────────────────────────────────────────────────────────
\subsection{UniSegments Database}

We leverage the UniSegments-1.0 database, a collection of morphologically segmented lexica covering 97 languages across 17 language families. Each entry provides:
\begin{itemize}
    \item \textbf{Word form}: the surface string (e.g., ``unbelievable'')
    \item \textbf{Morpheme boundaries}: character positions where morpheme splits occur (e.g., $\{2, 4, 11\}$ for ``un|be|liev|able'')
    \item \textbf{Morpheme types}: grammatical categories (prefix, root, suffix, etc.)
    \item \textbf{Language-specific affixes}: known prefixes and suffixes per language
\end{itemize}

The database aggregates resources including MorphoLex, MorphyNet, DeriNet, and language-specific morphological analyzers. We extract two key supervision signals:
\begin{enumerate}
    \item \textbf{Boundary hints}: Gold morpheme boundaries guide the DP lattice via soft rewards.
    \item \textbf{Cross-equivalence classes}: The \texttt{CROSS\_EQUIV} dictionary maps grammatical functions (PLURAL, PAST, NEGATION, etc.) to their surface realizations across languages, enabling cross-lingual alignment without parallel data.
\end{enumerate}

%% ─────────────────────────────────────────────────────────────────────────────
\subsection{N-gram Selection and Pruning}

\subsubsection{N-gram Extraction}
We frame candidate token selection as \emph{n-gram selection}. For each paragraph $p$ and position $i$, we extract all character n-grams $p[i:i+n]$ for $n \in [1, N_{\max}]$ (default $N_{\max}=14$), subject to legality checks. This exhaustive enumeration ensures no morphologically meaningful substring is missed.

\subsubsection{N-gram Legality}
An n-gram spanning $(i, j)$ is legal if it:
\begin{itemize}
    \item Respects grapheme boundaries (does not split combining characters, emoji, or diacritics from base characters)
    \item Does not overlap with protected spans (URLs, emails, numeric patterns)
    \item Passes the \texttt{is\_legal\_span} check in \texttt{ParagraphInfo}
\end{itemize}

\subsubsection{N-gram Filtering}
N-grams are filtered to create the \emph{potential tokens} set $\mathcal{T}_{\text{potential}}$. A candidate $\tau$ is rejected if:
\begin{itemize}
    \item $\text{freq}(\tau) < \texttt{min\_freq}$ (default 3)
    \item $\tau$ is all punctuation/whitespace with $\text{freq}(\tau) < 2 \times \texttt{min\_freq}$
    \item $\tau$ matches Wikipedia redirect patterns (\texttt{REDIRECT\_TOKEN\_RE})
    \item $\tau$ matches wiki noise patterns (\texttt{WIKI\_NOISE\_RE})
    \item $\tau$ has quote runs at edges with mixed scripts
    \item $\tau$ has $>2$ internal spaces and is not in the lexicon
\end{itemize}

\paragraph{Design rationale.}
Frequency thresholds eliminate noise from hapax legomena. The n-gram framing naturally captures morphemes of varying lengths: short n-grams $(n \le 3)$ capture common affixes like ``-ed'', ``-ing'', ``un-''; medium n-grams $(4 \le n \le 8)$ capture roots and stems; longer n-grams capture compound morphemes and reduplication patterns common in agglutinative languages.

\subsubsection{Statistical Costs}
For each potential n-gram $\tau$ we compute:
\begin{align}
\text{NLL}(\tau) &= -\log \frac{c_\tau + \alpha}{N_{|\tau|} + \alpha \cdot K_{|\tau|}}, \\
\text{PMI}_{\text{pen}}(\tau) &= -\log \frac{p_{\text{len}}(\tau)}{\prod_{c \in \tau} p(c)},
\end{align}
where $c_\tau$ is the count of $\tau$, $N_{|\tau|}$ is the total count of n-grams of length $|\tau|$, and $p(c)$ is the character probability. Length-conditioned NLL normalizes across n-gram lengths. PMI captures cohesion: n-grams with high PMI are more likely to be meaningful morphemes.

%% ─────────────────────────────────────────────────────────────────────────────
\subsection{DP Lattice and Cost Terms}

For each paragraph $x_{1:T}$ we build a lattice of legal spans. The DP state $D_t(k)$ stores the best cost of ending at position $t$ with morphological class $k$:
\[
\begin{aligned}
D_t(k)=\min_{(s,j,\tau)\in\mathcal{C}(t,k)}
\bigl[
    &D_s(j)
    + C_{\text{base}}(\tau) \\
    &+ C_{\text{morph}}(\tau,j,k,p)
\bigr],
\end{aligned}
\]
where $\mathcal{C}(t,k)$ enumerates legal spans ending at $t$ with compatible predecessor class $j$.

\paragraph{Base segmentation loss.}
Each span receives a linguistically informed cost:
\[
\begin{aligned}
C_{\text{base}}(\tau)=\ &
  \alpha \cdot \text{NLL}(\tau)
 + \beta \cdot \text{PMI}_{\text{pen}}(\tau)
 + \tau_{\text{len}} \cdot |\tau| \\
& - r_{\text{merge}}\bigl(|\tau|_{\text{grapheme}}-1\bigr)
  + p_{\text{short}}\mathbf{1}[|\tau|\le 2].
\end{aligned}
\]

\paragraph{UniSeg boundary rewards.}
The morphological cost term includes UniSeg-derived rewards:
\[
\begin{aligned}
C_{\text{morph}}(\tau,j,k,p)=\ &
  -r_{\text{UniSeg}}\;b_{\text{start}}(\tau,p)\;b_{\text{end}}(\tau,p) \\
& -r_{\text{affix}}(\tau,k)
  + r_{\text{trans}}(j\!\rightarrow\!k),
\end{aligned}
\]
where:
\begin{itemize}
    \item $b_{\text{start}}(\tau,p) = 1$ if position $\text{start}(\tau)$ aligns with a UniSeg morpheme boundary in word $p$
    \item $b_{\text{end}}(\tau,p) = 1$ if position $\text{end}(\tau)$ aligns with a UniSeg morpheme boundary
    \item $r_{\text{UniSeg}}$ (default 0.3) controls the strength of boundary alignment rewards
\end{itemize}

\paragraph{How UniSeg rewards promote morphological tokens.}
When both boundaries of a candidate token $\tau$ align with gold morpheme boundaries, $\tau$ receives a cost reduction of $-r_{\text{UniSeg}}$. This creates a systematic preference for tokens that correspond to complete morphemes:
\begin{itemize}
    \item A token ``walk'' in ``walking'' receives the full reward if UniSeg marks boundaries at positions 0 and 4.
    \item A token ``alki'' spanning the morpheme boundary receives \emph{no} reward, making it less likely to be selected.
    \item Over many words, tokens aligned with morpheme boundaries accumulate lower total cost and are preferentially added to the vocabulary.
\end{itemize}

%% ─────────────────────────────────────────────────────────────────────────────
\subsection{Pricing and Vocabulary Growth}

After segmenting the corpus we compute reduced costs for every candidate in $\mathcal{T}_{\text{potential}}$. The reduced cost for a token $\tau$ is:
\[
\begin{aligned}
\bar{c}(\tau)
=\ & C_{\text{create}}(\tau) \\
&- \sum_{(p,i)\in\mathcal{O}_\tau}
   \bigl(D^{(p)}_i-D^{(p)}_{i+|\tau|}\bigr),
\end{aligned}
\]
where $\mathcal{O}_\tau$ lists all occurrences of $\tau$. Tokens with $\bar{c}(\tau)<0$ are appended in batches of $k$ (default 67).

%% ─────────────────────────────────────────────────────────────────────────────
\subsection{Morphology Encoder}

\subsubsection{Feature Extraction}
Each token collects linguistically meaningful features:
\[
\begin{aligned}
\Phi(\tau)=\ &
 \text{char-}n\text{grams}_{(2,3,4)}(\tau)
\;\cup\; \text{Affixes}(\ell_\tau) \\
&\;\cup\; \text{CROSS\_EQUIV}(\tau)
\;\cup\; \text{UniSeg-boundaries}.
\end{aligned}
\]

The \texttt{CROSS\_EQUIV} features are crucial for cross-lingual alignment. If a token $\tau$ ends with a suffix belonging to a grammatical class (e.g., English ``-ing'' $\in$ PROGRESSIVE), it receives the class feature. This links:
\begin{itemize}
    \item English ``-ing'' (walking, talking)
    \item Turkish ``-yor'' (yürüyor, konuşuyor)
    \item Spanish ``-ando/-iendo'' (caminando, hablando)
\end{itemize}
as members of the same PROGRESSIVE class, even without parallel data.

\subsubsection{Convex GloVe Objective}

The original GloVe model learns both word and context embeddings jointly, leading to a nonconvex bilinear objective. By fixing feature embeddings $S \in \mathbb{R}^{F \times k}$ from our tokenizer, the optimization over token embeddings $U \in \mathbb{R}^{T \times k}$ becomes strictly convex.

Let $L \in \mathbb{R}^{T \times T}$ be a symmetric positive semidefinite Laplacian capturing morphological and cross-lingual relations. The convex objective is:
\begin{equation}\label{eq:glove}
\min_{U \in \mathbb{R}^{T\times k}}
\sum_{(t,f)\in\mathcal{I}} \frac{w_{tf}}{2} (u_t^\top s_f - X_{tf})^2
+ \frac{\lambda}{2} \mathrm{tr}(U^\top L U)
+ \frac{\gamma}{2} \|U\|_F^2,
\end{equation}
where $u_t^\top$ is the $t$th row of $U$, $s_f^\top$ the $f$th row of $S$, and $\gamma > 0$ ensures strong convexity.

\paragraph{Vectorized form.}
Let $u = \mathrm{vec}(U) \in \mathbb{R}^{Tk}$, and for $p=(t,f)$ define
\[
a_p^\top = \sqrt{w_{tf}}(e_t^\top \otimes s_f^\top) \in \mathbb{R}^{1\times Tk},
\quad
b_p = \sqrt{w_{tf}} X_{tf}.
\]
Stacking these rows forms $A \in \mathbb{R}^{m\times Tk}$ and $b \in \mathbb{R}^m$, where $m = |\mathcal{I}|$. Let
\[
B = \lambda (L \otimes I_k) + \gamma I_{Tk} \succeq \gamma I_{Tk}.
\]
Then (\ref{eq:glove}) is equivalent to the quadratic problem:
\begin{equation}\label{eq:quad}
\min_{u \in \mathbb{R}^{Tk}} f(u) = \frac{1}{2}\|A u - b\|_2^2 + \frac{1}{2} u^\top B u.
\end{equation}

\paragraph{How GloVe promotes morphological tokens.}
The Laplacian term $\mathrm{tr}(U^\top L U)$ encodes the \texttt{CROSS\_EQUIV} structure:
\begin{enumerate}
    \item For each grammatical class $c$ (PLURAL, PAST, etc.), we build a graph where nodes are tokens matching class $c$ and edges connect tokens within the same class.
    \item The Laplacian $L^{(c)} = D^{(c)} - W^{(c)}$ penalizes embeddings that drift apart: $\mathrm{tr}(U^\top L^{(c)} U) = \sum_{i \sim j} \|u_i - u_j\|^2$.
    \item Minimizing this term pulls same-class morphemes together in embedding space.
\end{enumerate}

Combined with the reconstruction term, tokens with similar morphological features (n-grams, affixes) and belonging to the same \texttt{CROSS\_EQUIV} class receive similar embeddings. These embeddings then inform the affix rewards $r_{\text{affix}}$ in the DP lattice, creating a feedback loop that reinforces morphologically coherent tokenization.

\subsubsection{Sparse Laplacian Implementation}

The Laplacian $L \in \mathbb{R}^{T \times T}$ is \emph{never built as a dense matrix}. Instead:
\begin{enumerate}
    \item We collect \textbf{equivalence sets} $\mathcal{E}_c$ of token indices per class.
    \item We convert these to \textbf{sparse edge lists} $(i, j, w_{ij})$ connecting same-class tokens.
    \item The product $LU$ is computed via scatter operations: $(LU)_i = \sum_{j \sim i} w_{ij}(u_i - u_j)$.
    \item The linear system $(G + \lambda L)U = c$ is solved via \textbf{conjugate gradient}, requiring only matrix-vector products.
\end{enumerate}
This enables scaling to large vocabularies ($T > 10^4$) without materializing the $T^2$ matrix.

%% ─────────────────────────────────────────────────────────────────────────────
\subsection{Convergence Guarantees}

The quadratic objective in (\ref{eq:quad}) is $\gamma$-strongly convex with an $L$-Lipschitz gradient; here $\gamma$ comes from the $\ell_2$ term and a loose bound is $L \le \lambda_{\max}(A^\top A) + \lambda \lambda_{\max}(L) + \gamma$. Gradient descent with step size $\alpha \in (0, 2/L)$ converges linearly to the unique optimum. Using $\alpha^\star = 2/(L+\gamma)$ yields rate $(\kappa-1)/(\kappa+1)$ with condition number $\kappa = L/\gamma$. Proofs are omitted for brevity.

%% ─────────────────────────────────────────────────────────────────────────────
\subsection{Iteration Update and Combined Objective}

Each outer iteration performs:
\begin{enumerate}
    \item \textbf{DP segmentation}: run the lattice recurrence with the current vocabulary, yielding optimal segmentations and DP potentials $D$ for every paragraph.
    \item \textbf{Column-generation pricing}: compute reduced costs $\bar{c}(\tau)$, append the most negative tokens (subject to the vocab budget), and drop stale entries.
    \item \textbf{Morphology refinement}: rebuild $X$ with the updated vocabulary, minimize $\mathcal{L}_{\text{morph}}(U)$ via gradient descent (guaranteed convergent), and refresh the affix/transition rewards inside the DP lattice.
\end{enumerate}
Overall we optimize:
\[
\min_{\text{vocab},\,U}
\Biggl[
\sum_{p}\min_{\text{seg}(p)}
\sum_{\tau\in\text{seg}(p)}
\bigl(C_{\text{base}}(\tau)+C_{\text{morph}}(\tau)\bigr)
\Biggr]
+\mathcal{L}_{\text{morph}}(U).
\]

%% ─────────────────────────────────────────────────────────────────────────────
\subsection{Vocabulary Budget Enforcement}

After training, if a \texttt{vocab\_budget} is specified, we enforce it via Lagrangian relaxation. A bisection search over penalty $\lambda \in [\lambda_{\text{lo}}, \lambda_{\text{hi}}]$ finds the threshold where exactly the budget number of tokens have negative penalized reduced cost.

%% ─────────────────────────────────────────────────────────────────────────────
\subsection{Summary}

Our tokenizer:
\begin{itemize}
    \item Leverages UniSegments-1.0 for morpheme boundary supervision and cross-lingual equivalence classes.
    \item Frames candidate selection as n-gram extraction with linguistically motivated filtering.
    \item Uses UniSeg boundary rewards to systematically prefer tokens aligned with gold morpheme boundaries.
    \item Learns GloVe-style embeddings with a Laplacian regularizer that provably converges to a unique global optimum.
    \item The Laplacian pulls same-class morphemes together across languages, enabling cross-lingual transfer without parallel data.
    \item Grows the vocabulary through column-generation pricing with guaranteed improvement at each iteration.
\end{itemize}
