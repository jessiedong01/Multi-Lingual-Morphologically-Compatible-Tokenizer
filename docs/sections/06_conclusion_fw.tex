\section{Conclusion}

This work introduced a morphology-aware tokenization framework that incorporates cross-lingual consistency constraints into the learning process. By aligning equivalent morphemes across languages, the model produces more interpretable segmentations and induces a more structured embedding space. Our experiments show that the approach yields measurable improvements for morphologically rich languages, while also revealing important typological limits in analytic languages where affixal information is sparse. These findings underscore the role of linguistic structure in tokenizer design and highlight the potential of simple, interpretable objectives to improve cross-lingual generalization. Future work will extend this framework to low-resource languages and explore richer, automatically discovered equivalence mappings to broaden the applicability of morphology-aware tokenization.
