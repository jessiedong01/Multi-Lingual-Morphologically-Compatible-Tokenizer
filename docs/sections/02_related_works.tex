
\section{Related works}
Tokenization chooses the units through which a model represents linguistic structure. While most modern systems approach tokenization as a frequency-based compression problem, research in unsupervised cross-lingual representation learning shows that meaningful linguistic structure—morphology, syntax, and semantic neighborhoods—can be recovered even without parallel data. These two strands of work motivate our approach: tokenization should be guided not only by frequency, but by the geometric and morphological structure that underlies cross-lingual consistency.

\paragraph{Unsupervised cross-lingual alignment}
A central line of work studies how independently trained monolingual embeddings can be mapped into a shared space without supervision. Early methods assume that embedding spaces for different languages exhibit similar geometric structure, allowing a linear mapping to align them \cite{mikolov2013exploiting}. The discovery that optimal mappings are approximately orthogonal led to the widespread use of the Procrustes solution \cite{xing2015normalized}. Conneau et al.\ demonstrate that such mappings can be learned \emph{without bilingual data} by combining adversarial training with an orthogonality constraint, and refining the resulting mapping via a synthetic dictionary and Procrustes iteration \cite{conneau2018word}. Their adversarial objective encourages the mapped source distribution to match the target distribution, while cross-domain similarity local scaling (CSLS) mitigates the hubness problem that arises in high-dimensional spaces. This framework achieves state-of-the-art unsupervised performance on word translation and cross-lingual retrieval tasks, even for distant language pairs such as English–Russian and English–Chinese.\footnote{See detailed results and methodology in \cite{conneau2018word}.}

These results establish two important themes for our work. First, linguistic structure can emerge from geometric constraints when the optimization objective is designed appropriately. Second, cross-lingual consistency arises not from supervision but from smoothness, locality, and distributional alignment—principles that similarly motivate morphology-aware tokenization.

\paragraph{Morphology-aware tokenization}
Recent morphology-aware tokenizers aim to align subwords with meaningful morphological units rather than surface-level frequency statistics. MorphPiece \cite{Jabbar2023}, MorphBPE \cite{asgari2025morphbpe}, and MorphTok \cite{brahma2025morphtok} integrate analyzers or lexicons into the merge process, producing segmentations that better preserve stems, affixes, and inflectional paradigms. Large-scale evaluations across diverse language families show that morpheme-aligned tokenizers reduce sequence length, improve translation and tagging accuracy, and reduce model redundancy \cite{arnett2025evaluating}. Other work shows that embeddings derived from morphology-aware segmentations yield more stable and interpretable representations, and improve convergence in generative models \cite{Conneau2020,Jabbar2023}. However, these approaches typically remain heuristic or language-bound: MorphPiece blends an external segmenter with BPE scoring; MorphBPE tweaks merge scores with boundary penalties; MorphTok leans on rules and lexica for Indic languages. None learns a shared embedding geometry or provides a systematic, cross-lingual control over the balance between morphology and frequency.

\paragraph{Linguistic information beyond morphology}
Beyond morphology, several studies incorporate syntactic information into tokenization or representation learning. POS-aware or syntactic embedding models \cite{azmi2025token,hlaing2022improving,qi2025symmetric} improve tagging and translation performance by enriching token representations with grammatical context. Analyses of tokenization effects \cite{toraman2023impact,mielke2021between} highlight that segmentation choices implicitly shape how models represent syntactic and semantic structure. Tokenizers that split morphemes or cross POS boundaries introduce redundancy and degrade generalization, especially in morphologically rich languages. These findings reinforce that tokenization must be treated as a modeling decision rather than a preprocessing step.


\paragraph{Retrofitting and semantic post-processing}
A parallel line of work improves lexical representations by enforcing semantic structure \emph{after} embeddings are trained, rather than modifying the training objective. Retrofitting~\cite{faruqui2015retrofitting} provides a post-hoc graph-based method that adjusts pretrained word embeddings so that lexicon-linked words (e.g., synonyms, hypernyms, paraphrases) move closer together in vector space. Unlike methods that modify the training loss or require model-specific constraints, retrofitting operates by running belief propagation over a semantic graph, typically derived from resources such as WordNet, FrameNet, or PPDB, and updating each vector to minimize its distance to both its original embedding and its graph neighbors. This process is convex, efficient, and agnostic to how the initial embeddings were obtained, making it applicable across models and languages. Empirically, Faruqui et al.\ demonstrate substantial gains on lexical similarity, analogy, and sentiment tasks across multiple embedding families (GloVe, skip-gram, GC, multilingual vectors), with especially large improvements for PPDB-based constraints.\footnote{See detailed formulation in \cite{faruqui2015retrofitting}.} \emph{Conceptually}, retrofitting illustrates how injecting structured linguistic knowledge into a representation space can sharpen semantic neighborhoods and enforce relational coherence, but it remains a \textit{post-processing} method that does not shape the tokenization process itself. In contrast, our approach integrates morphological and lexical structure directly into the tokenizer’s optimization objective, rather than adjusting embeddings after the fact. This shifts linguistic structure from an optional correction step to a foundational design principle for segmentation.


\paragraph{Positioning of this work}
MorphPiece linearly blends an external morph segmenter with a statistical tokenizer (weight $\alpha$); MorphBPE alters BPE merge scores with morphology-aware penalties; MorphTok is rule/lexicon-heavy for Indic languages. All three add morphology, but none learns an embedding geometry that feeds back into decoding. Our approach differs in three ways: (1) \textbf{explicit, tunable trade-off}: the DP decoder keeps statistical costs (NLL, PMI, length) and adds parameterized morphology rewards (UniSeg boundaries, affix/transition bonuses), so we can sweep morph bias and quantify its effect on segmentation and perplexity; (2) \textbf{convex cross-lingual encoder}: a GloVe+Laplacian objective (cf. \cite{Conneau2020}) clusters same-function morphemes across languages and feeds those similarities back as rewards, injecting cross-lingual signal and enforce reward consistency even when monolingual frequency is ambiguous and weak compared to high information language which provides an anchor. 