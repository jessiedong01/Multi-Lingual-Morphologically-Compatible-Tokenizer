\section{Introduction}
\label{sec:introduction}

Tokenization determines how text is divided into smaller units that language models can process. These units, known as tokens, form the basis of every downstream computation because embeddings, attention, and predictions all operate on them. Most modern tokenizers, including Byte Pair Encoding (BPE)~\cite{Sennrich2016} and SentencePiece~\cite{KudoRichardson2018}, rely on frequency-based merges of character sequences. This approach is efficient and easy to scale, but it treats language as a sequence of symbols rather than a structured system governed by grammar and morphology.

For languages like English, where much grammatical information is expressed through separate words, this simplification is often sufficient. However, in many other languages such as Turkish, Finnish, or Arabic, a single word can represent an entire phrase by combining a stem with several affixes. Frequency-based tokenizers tend to split these words at arbitrary points or merge unrelated parts, resulting in tokens that do not align with linguistic meaning. These errors increase sequence length, obscure grammatical relationships, and make it harder for models to generalize across languages~\cite{Kaya2024, Toraman2023}.

Morphology provides a more structured perspective. A \textbf{morpheme} is the smallest unit of meaning or grammatical function, and words are typically formed by combining several morphemes~\cite{Saleva2021}. When tokenization respects these boundaries, the resulting vocabulary captures real linguistic patterns rather than surface co-occurrences. Morphology-aware segmentation produces tokens that are more consistent and informative, improving translation quality and cross-lingual representation \cite{Conneau2020, Jabbar2023}. In contrast, frequency-based methods often learn redundant subwords that repeat similar meanings in slightly different forms, wasting vocabulary capacity and obscuring meaningful relationships.

We build a two-part tokenizer that exploits morphology in complementary ways. First, a \textbf{morphology encoder} learns token embeddings from linguistic features (character $n$-grams, affixes, and cross-lingual equivalence classes) using a convex GloVe objective with a Laplacian regularizer. This creates a shared, well-behaved space where same-function morphemes are close across languages, enabling cross-lingual consistency and providing richer syntactic/semantic cues for downstream models~\cite{Conneau2020}. Second, a \textbf{morphology-aware decoder} directly rewards morpheme-respecting spans using UniSeg boundary hints and affix signals inside a dynamic-programming lattice, pushing segmentation toward true morpheme boundaries rather than arbitrary frequency splits.

The encoder's geometry feeds the decoder: spans whose embeddings align with known morphological classes receive lower costs, while UniSeg boundaries and affix cues further reduce the segmentation loss. This coupling preserves grammatical structure while maintaining the scalability required for large-scale models.

Currently, canonical tokenizers segment tokens poorly into morphological elements. For example, \textbf{Muselmanin} means “of a Muslim,” and it should be separated morphologically as \textit{Muselman} + \textit{-in}. However, canonical tokenizers often segment the word as ["Mus", "elman", "in"], or ["Mus", "el", "man", "in"], completely losing the morphologically correct structure. This property is also demonstrated in Figure~\ref{fig:workflow}, which showcases canonical tokenizers' struggle on large corpora.

Experiments across three typologically diverse languages show that this tokenizer identifies more true morphological boundaries, reduces token fragmentation, and improves cross-lingual consistency compared to frequency-based baselines. These results indicate that embedding linguistic structure directly into the tokenization process leads to more interpretable and efficient representations. More broadly, they show that tokenization is not merely a preprocessing step but a modeling choice that affects how language structure is captured by neural networks.
