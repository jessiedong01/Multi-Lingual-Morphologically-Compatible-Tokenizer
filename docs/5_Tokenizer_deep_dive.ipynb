{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab47349c-2da9-4083-847c-e0d1a07c31fe",
   "metadata": {},
   "source": [
    "# Tokenizer deep-dive üßê\n",
    "*(contact: arjo@stanford.edu)*\n",
    "\n",
    "The Tokenizer pulls together all we've learned so far and builds our vocabulary.\n",
    "\n",
    "Let's dive into every step while building our vocabulary and what it does ..!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd7f5dd-d695-4337-854b-0c949b7c4a14",
   "metadata": {},
   "source": [
    "## Setup dataset for tokenization\n",
    "Let's start by choosing a couple of languages and setting up a dataset we can train our tokenizer on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca9f69-66c3-40bb-afdb-10aea41e609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_wikiann_corpus\n",
    "# --- 1. Define and Load the Corpus ---\n",
    "# Define the languages we want to train on.\n",
    "# We map short codes (e.g., 'en') to their full names for clarity.\n",
    "lang_codes = { \n",
    "    'en': 'English', 'da': 'Danish',\n",
    "    #'zh': 'Chinese',# 'fr': 'French',\n",
    "    #'tr': 'Turkish', 'ru': 'Russian', 'ja': 'Japanese',\n",
    "    #'ar': 'Arabic', 'ta': 'Tamil', 'xh': 'Xhosa',\n",
    "    #'zu': 'Zulu', 'tk': 'Turkmen'\n",
    "}\n",
    "\n",
    "# Load the training data from the Hugging Face 'wikiann' dataset.\n",
    "num_paragraphs = 700\n",
    "corpus_texts, corpus_langs = load_wikiann_corpus(lang_codes, per_lang=num_paragraphs)\n",
    "print(f\"Number of sentences: {len(corpus_texts)} across {len(lang_codes)} languages\")\n",
    "print(f\"Let's print a few examples:\")\n",
    "for i in range(len(lang_codes)):\n",
    "    txt = corpus_texts[10+i*num_paragraphs]\n",
    "    lang = corpus_langs[10+i*num_paragraphs]\n",
    "    print(f\"\\tLanguage: {lang}, Text: {txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e919e573-eaf1-47b6-84d3-6f75425d2113",
   "metadata": {},
   "source": [
    "---\n",
    "Now we have an small interesting multi-lingual dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d315556-f291-4c59-b8a9-b68f905995b0",
   "metadata": {},
   "source": [
    "### ScalableTokenizer.__init__(...)\n",
    "This is the args description\n",
    "```\n",
    "Args:\n",
    "    max_token_len (int): The maximum length in characters for any learned token.\n",
    "    min_freq (int): A candidate token must appear at least this many times\n",
    "        in the corpus to be considered for the vocabulary.\n",
    "    alpha (float): A weighting factor for the token's negative log-likelihood cost.\n",
    "    beta (float): A weighting factor for the token's PMI-based cohesion penalty.\n",
    "    tau (float): A weighting factor for the token's length penalty.\n",
    "    top_k_add (int): The number of best new tokens to add to the vocabulary\n",
    "        in each training iteration.\n",
    "    vocab_budget (int | None): The target size for the final vocabulary. If None,\n",
    "        no budget is enforced.\n",
    "    lambda_lo (float): The lower bound for the Lagrangian multiplier used\n",
    "        in vocabulary budgeting.\n",
    "    lambda_hi (float): The upper bound for the Lagrangian multiplier.\n",
    "```\n",
    "There's two groups of variables. Once related to hard constraints on the vocabulary (`max_token_len`, `min_freq`, `vocab_budget`) and hyperparameters (`alpha`, `beta`, `tau`, `lambda_lo`, `lambda_hi` `top_k_add`). For now, we don't modify most of the hyperparameters, this is something you can look into as the default values are not optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c925a87-c4f8-4fd5-8f6c-ed53bd85bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import ScalableTokenizer\n",
    "# --- 2. Initialize and Configure the Tokenizer ---\n",
    "# Create an instance of our ScalableTokenizer.\n",
    "# We configure its core behavior with these parameters:\n",
    "tokenizer = ScalableTokenizer(\n",
    "    max_token_len=12,   # The longest possible token (in characters)\n",
    "    min_freq=7,         # A word must appear at least 7 times in the corpus\n",
    "    top_k_add=8,        # In each training step, add the 8 \"best\" new tokens to the vocabulary\n",
    "    vocab_budget=500    # The target size for our final vocabulary (number unique tokens)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56f35f4-51c6-441d-8b23-cbcf2217cc2b",
   "metadata": {},
   "source": [
    "---\n",
    "Great! Now we have our tokenizer.\n",
    "However, for design purposes we have not yet filled in any linguistic information.\n",
    "Thus, it is not \"linguisticly-aware\" yet. We do that after initializing our tokenizer using the `ScalableTokenizer.set_feature_models` function. This will initialize a `LinguisticModels` object that will be used for linguistic awareness during tokenization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15c73c-f64e-4236-82b0-dc2d91fd4804",
   "metadata": {},
   "source": [
    "## LinguisticModels\n",
    "This is the cores of linguistic awareness in our tokenizer. It uses linguistic signals such as lexical, sequential, morphological, and syntactic information to define a cost for each token. This can then be used downstream to pick the most valuable tokens out (i.e. see `ScalableTokenizer._dp_decode`).\n",
    "\n",
    "Examples include:\n",
    "- Keep `\"New York\"` as one token.\n",
    "- Sequence of capitalized words might be a name, e.g. `\"Jane Doe\"`\n",
    "- Morphologically consistant: e.g. `\"walking\"` consists of `\"walk\"` and `\"-ing\"`, so it will be scored higher as a full token instead of splitting it up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768b59c-5929-4e2b-a389-28bb14a1e31b",
   "metadata": {},
   "source": [
    "## LinguisticModels.init(...)\n",
    "This is the args description\n",
    "```\n",
    "\"\"\"\n",
    "Attributes:\n",
    "    lexicon (dict): A dictionary mapping known tokens to a score (reward).\n",
    "    mwe (set): A set of known multi-word expressions.\n",
    "    ne_gaz (dict): A named entity gazetteer, mapping entity types (e.g., \"LOC\")\n",
    "        to sets of known entities.\n",
    "    token_bigram (dict): A dictionary defining costs for transitions between\n",
    "        token classes (e.g., from \"InitCap\" to \"InitCap\").\n",
    "    lm_token_prob (callable | dict): An external language model providing token\n",
    "        probabilities.\n",
    "    paragraph_lang (callable): A function that returns the language for a given\n",
    "        paragraph index.\n",
    "    gamma_boundary (float): A penalty for changing token classes, which\n",
    "        encourages sequences of similar token types.\n",
    "    mu_morph (float): The weight applied to the score from the morphology\n",
    "        encoder.\n",
    "    rho_group (float): A small reward (negative cost) for tokens that contain\n",
    "        known prefixes or suffixes.\n",
    "    morph_encoder (MorphologyEncoder | None): An instance of the morphology\n",
    "        encoder, which is trained to score the morphological \"fit\" of a\n",
    "        token for a given language.\n",
    "\"\"\"\n",
    "```\n",
    "The initialization allows us to manually bias our tokenizer towards certain token constructs.\n",
    "Let's test this by setting `lexicon`, `mwe`, and `token_bigram`.\n",
    "We skip `lm_token_prob`, `paragraph_lang` is handled later on, `gamma_boundary`, `mu_morph`, & `rho_group` are hyperparameters, and `morph_encoder` is created later on in `tokenizer._initialize_stats_and_vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1132d-78bc-4b55-a023-e56af746a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Set up Linguistic \"Hints\" ---\n",
    "# We can give the tokenizer extra knowledge to improve its accuracy.\n",
    "    \n",
    "# A 'lexicon' of known multi-word phrases. The numbers are scores; higher is better.\n",
    "# This encourages \"New York\" to be one token instead of two (\"New\", \"York\").\n",
    "lex = {\"New York\": 2.0, \"San Jose\": 1.0, \"‚Äôs\": 0.5, \"'s\": 0.5}\n",
    "\n",
    "# A 'named entity' (ne) gazetteer. This lists known entities.\n",
    "# We're telling it that \"New York\", \"Berlin\", and \"Êù±‰∫¨\" are locations (LOC).   \n",
    "ne_gaz  = {\"LOC\": {\"New York\", \"Berlin\", \"Êù±‰∫¨\"}}\n",
    "\n",
    "# 'Token bigrams' (tb) define costs for sequences of token types.\n",
    "# A negative cost is a \"reward\". This encourages sentences to start with a\n",
    "# capitalized word and for capitalized words to follow each other (like in a name).\n",
    "token_bigram  = {\n",
    "    (\"<BOS>\", \"InitCap\"): -0.2,\n",
    "    (\"InitCap\", \"InitCap\"): -0.3,\n",
    "    (\"NUM\", \"NUM\"): -0.15,\n",
    "}                   \n",
    "                    \n",
    "# Now, we apply these linguistic models and tune some internal algorithm weights.\n",
    "tokenizer.set_feature_models(\n",
    "    lexicon=lex,\n",
    "    ne_gaz=ne_gaz,\n",
    "    token_bigram=token_bigram,\n",
    "    gamma_boundary=0.06,\n",
    "    mu_morph=0.25,\n",
    "    rho_group=0.06\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7834d-c23d-4204-986b-911f1b4eaaef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ea464-5457-4508-af04-2b595562b82f",
   "metadata": {},
   "source": [
    "## ScalableTokenizer.train(...)\n",
    "This is where the magic happens, all the functions we've built so far gets pulled together and utilized to build our vocabulary.\n",
    "Below we will spell out every step.\n",
    "\n",
    "Importantly, this is when we provide the text, language-id per sample, and how many iterations we want to run it for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22083c0f-519a-4b7e-abdd-69328638b352",
   "metadata": {},
   "source": [
    "## ScalableTokenizer._initialize_stats_and_vocab\n",
    "This function is the \"data ingestion and analysis\" phase of the tokenizer. Its primary goal is to find every potential token in the corpus and assign it two fundamental statistical scores: one measuring its rarity and another measuring its internal cohesion.\n",
    "\n",
    "The process can be broken down into six main steps:\n",
    "\n",
    "#### 1. Substring Enumeration and Counting\n",
    "\n",
    "First, the function performs a brute-force scan of the entire corpus. It slides a window of every possible length (from 1 up to `max_token_len`) across the text and counts the occurrences of every single \"legal\" substring. A substring is legal if it respects Unicode boundaries and doesn't improperly split protected entities like URLs (as determined by `utils.is_legal_span`).\n",
    "\n",
    "This step produces two key objects:\n",
    "\n",
    "* `char_count`: A simple frequency count of every individual character.\n",
    "* `substr_count`: A frequency count of every valid substring found in the text.\n",
    "\n",
    "NOTE: this can get expensive memory-wise for large corpora - how can we resolve that? ‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Candidate Filtering\n",
    "\n",
    "The raw `substr_count` contains a massive number of substrings, many of which are junk or noise. This step acts as a filter, creating a cleaner set of `_potential_tokens` by applying several heuristic rules:\n",
    "\n",
    "* **Frequency:** Must appear at least `min_freq` times.\n",
    "* **Noise:** Filters out tokens that are likely wiki-formatting (`''`, `==`), redirect commands, or just punctuation.\n",
    "* **Structure:** Filters out tokens with too many internal spaces or strange mixtures of scripts.\n",
    "\n",
    "This dramatically reduces the number of candidates the algorithm needs to consider.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Indexing Occurrences\n",
    "\n",
    "For speed, the algorithm needs to be able to quickly find every location a potential token appears. This step builds an index (a dictionary called `_token_occurrences`) that maps each potential token to a list of all its starting positions in the corpus.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Calculating Statistical Costs (The Math) üß†\n",
    "\n",
    "This is the heart of the function. For every potential token, we calculate two scores that form its \"base cost.\"\n",
    "\n",
    "**Token Rarity: The Negative Log-Likelihood Cost (`_nll`)**\n",
    "\n",
    "This score measures how rare a token is, conditioned on its length. The core idea is that more frequent tokens should have a lower cost. We use the negative log-likelihood, which is a standard way to turn probabilities into costs in machine learning.\n",
    "\n",
    "First, we calculate the smoothed probability of a token t of length L:\n",
    "$$P(t|L)=\\frac{\\texttt{count}(t) + \\alpha_{\\texttt{len}}}{\\left(\\sum_{t'\\; \\text{where}\\; \\texttt{len}(t')=L}\\;\\texttt{count}(t')\\right) + \\alpha_{\\texttt{len}}\\cdot N_L} $$\n",
    "\n",
    "Where:\n",
    "* $\\texttt{count}(t)$ is the frequency of our specific token $t$.\n",
    "* The sum in the denominator is the total count of all tokens that have the same length L.\n",
    "* $\\alpha_{\\texttt{len}}$ is a smoothing constant (additive or Laplace smoothing) to prevent probabilities of zero for unseen tokens.\n",
    "* $N_L$ is the number of unique token types of length L.\n",
    "\n",
    "The final cost is the negative log of this probability. Tokens that are common for their length will have a high probability and thus a low cost.\n",
    "$$ \\text{cost}_{\\text{NLL}}(t) = ‚àí\\log(P(t‚à£L))$$\n",
    "\n",
    "**Token Cohesion: The PMI-like Penalty (`_pmi_pen`)**\n",
    "\n",
    "This score measures how \"gluey\" or cohesive a token's characters are. It answers the question: \"Do these characters appear together more often than we'd expect by chance?\" A high cohesion score suggests the string is a meaningful unit (like \"ing\") rather than a random sequence of letters.\n",
    "\n",
    "This is based on **Pointwise Mutual Information (PMI)**. The formula is:\n",
    "$$\\text{PMI-score}(t) =\\log\\left(\\frac{P(t|L)}{\\prod^L_{i=1}P(c_i)}\\right) $$\n",
    "Where:\n",
    "* $P(t‚à£L)$ is the length-conditioned probability we just calculated above.\n",
    "* $P(c_i)$ is the probability of the i-th character in the token, derived from the initial `char_count`.\n",
    "* The product $\\prod P(c_i)$ represents the probability of seeing that sequence of characters if they were all statistically independent.\n",
    "\n",
    "If the score is high, it means the numerator is much larger than the denominator‚Äîthe token appears far more frequently than chance would suggest, making it a cohesive unit. The final `_pmi_pen` is the **negative** of this score, so that highly cohesive tokens get a **cost reduction (a reward)**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Initializing the Vocabulary\n",
    "\n",
    "The tokenizer needs a starting vocabulary. This step simply initializes the vocabulary with all single characters seen in the corpus. This provides a crucial fallback: if no multi-character tokens can be formed, the system can always represent the text as a sequence of individual characters.\n",
    "\n",
    "NOTE: How would this vocabulary handle large amounts of unique unicode symbols? ‚≠ê\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Training the Morphology Encoder\n",
    "\n",
    "Finally, using the collected statistics (`_potential_tokens` and `_token_occurrences`), the function trains the `MorphologyEncoder`. This component learns the sub-word patterns (common prefixes, suffixes, and character n-grams) of each language. Its math is self-contained (involving matrix factorization of a PPMI matrix), but it relies on the data prepared in the preceding steps to do its job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84481fd3-a9da-4210-ae47-90ede6d433a4",
   "metadata": {},
   "source": [
    "## MorphologyEncoder.fit\n",
    "\n",
    "The purpose of this function is to learn the morphological \"shape\" of words. It does this by representing tokens based on their sub-word features (character n-grams and affixes) and then using matrix factorization to discover the most important patterns. The final output is a low-dimensional vector for each token, where tokens with similar morphological patterns (like \"running\" and \"swimming\") are close to each other in the vector space.\n",
    "\n",
    "The process unfolds in six key steps:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Determine Primary Language for Each Token\n",
    "\n",
    "A token (especially a name or a loanword) might appear in texts from multiple languages. This initial step simply assigns each token to a single, primary language based on where it appears most frequently. This ensures that a token like \"start\" is analyzed using English morphology rules, even if it appears once in a German text.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Build the Feature Matrix $X$\n",
    "\n",
    "This is the foundational step where we convert raw text into a numerical matrix. The matrix X represents the relationship between tokens and their morphological features.\n",
    "\n",
    "* **Rows:** Each row corresponds to a unique token t from the corpus.\n",
    "* **Columns:** Each column corresponds to a unique feature f. Features are things like character bigrams (`'th'`, `'ng'`), trigrams (`'ing'`), and known affixes (`'$suf:ing'`).\n",
    "* **Values:** The cell $X_{tf}$ contains an integer count of how many times feature $f$ appears in token $t$.\n",
    "\n",
    "For example, for the token `'running'`, the row would have non-zero values in the columns corresponding to features like `'ru'`, `'un'`, `'nn'`, `'ni'`, `'in'`, `'ng'`, `'run'`, `'unn'`, `'nni'`, `'nin'`, `'ing'`, and `'$suf:ing'`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Calculate the Positive Pointwise Mutual Information (PPMI) Matrix\n",
    "\n",
    "Using raw counts ($X$) is not ideal because common features (like the character `'e'`) aren't very informative. We want to find features that are surprisingly common for a given token. This is what **Pointwise Mutual Information (PMI)** measures.\n",
    "\n",
    "The formula for PMI between a token t and a feature f is:\n",
    "$$\\text{PMI}(t,f)=\\log_2\\left(\\frac{P(t,f)}{P(t)P(f)}\\right)$$\n",
    "\n",
    "Let's break this down:\n",
    "\n",
    "* $P(t,f)$: The joint probability of seeing the token $t$ and the feature $f$ together. In the code, this is calculated as `P_xy`, where each cell is the count from $X$ divided by the total sum of all counts in $X$.\n",
    "* $P(t)$: The marginal probability of seeing token $t$. This is the sum of row $t$ in $X$ divided by the total sum (`P_x`).\n",
    "* $P(f)$: The marginal probability of seeing feature $f$. This is the sum of column $f$ in $X$ divided by the total sum (`P_y`).\n",
    "\n",
    "The ratio $\\frac{P(t,f)}{P(t)P(f)}$ tells us how much more likely the feature and token are to co-occur than if they were independent. A large ratio means the feature is highly indicative of that token.\n",
    "\n",
    "The code then computes **Positive PMI (PPMI)** by taking `max(0, PMI)`. This discards negative associations (where features co-occur less than chance), as they are often noisy and not useful for this task. The result is a matrix `PPMI` of the same shape as $X$, but with values that represent feature informativeness rather than raw counts.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Factorize the PPMI Matrix to Get Vectors (The Math)\n",
    "\n",
    "We now have a large, sparse `PPMI` matrix. Our goal is to create short, dense vectors (embeddings) for each token. This is a dimensionality reduction problem. The code uses **Eigendecomposition**, a powerful technique from linear algebra.\n",
    "\n",
    "* **Create a Token-Token Similarity Matrix ($G$):** The code first computes $G=PPMI\\times PPMI^T$, $G=(G+G^T)\\cdot 0.5$. This results in a square matrix where each cell $G_{ij}$ represents the similarity between token $i$ and token $j$. If two tokens share many of the same informative features, their corresponding value in $G$ will be high. This is known as a Gram matrix.\n",
    "* **Eigendecomposition:** The code then performs eigendecomposition on G. This breaks the matrix down into its constituent eigenvalues and eigenvectors: $G=Q\\Lambda Q^{‚àí1}$\n",
    "* **Eigenvectors ($Q$):** These are the principal components of the matrix. They represent the fundamental directions of variation in the data‚Äîin this case, the most important axes of morphological similarity.\n",
    "* **Eigenvalues ($\\Lambda$):** These are scalar values that indicate how much variance is explained by each eigenvector. A large eigenvalue means its corresponding eigenvector is very important.\n",
    "* **Construct the Embedding Matrix ($V$):** We don't need all the eigenvectors, only the most important ones. The code selects the top `k` eigenvectors corresponding to the `k` largest eigenvalues. The final token embedding for each token is constructed by taking its corresponding row from the top `k` eigenvectors, and scaling each component by the square root of its associated eigenvalue. This is a standard method that yields embeddings where the squared distance between vectors approximates the similarity relationship in the original data.\n",
    "* **Normalization:** Finally, each vector in $V$ is normalized to have a unit length of 1. This is crucial because it allows us to use the dot product as a direct measure of **cosine similarity**. When comparing two unit vectors, their dot product is exactly the cosine of the angle between them, providing a pure measure of similarity irrespective of vector magnitude.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Store Learned Vectors and Language Prototypes\n",
    "\n",
    "The result of the factorization is stored:\n",
    "* `token_vec`: A dictionary mapping each token string to its learned dense vector (a row from $V$).\n",
    "* `lang_proto`: A dictionary mapping each language to a \"prototype\" vector. This prototype is calculated by **averaging the vectors of all tokens** belonging to that language. It represents the morphological center-of-gravity for that language in the learned vector space.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Pre-calculate Counts for Consistency Bonus\n",
    "\n",
    "This is a final preparatory step. It iterates through the tokens and counts, for each cross-lingual morphological category (like `PLUR` for plural), how many different languages have tokens belonging to that category. This pre-computation makes the `consistency_bonus` calculation during the main tokenization process much faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a7ed0-78a5-441c-a242-3dbb63842f81",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Phew that was a lot ...! let's code it up, it's just one LOC (should take ~2 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec58b3e-a8a8-4a8d-bf8b-3c302e56ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._initialize_stats_and_vocab(corpus_texts, corpus_langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178eee4d-56a7-49d7-8617-6a2bacedce26",
   "metadata": {},
   "source": [
    "### ScalableTokenizer.train - Loop phase\n",
    "The training loop continuously alternates between two main phases: the **Primal Step** (solving the problem with the current vocabulary) and the **Pricing Step** (finding new, better vocabulary to add).\n",
    "\n",
    "---\n",
    "\n",
    "**Phase 1: The Primal Step - Finding the Best Path with `_dp_decode` üó∫Ô∏è**\n",
    "\n",
    "In each iteration, the first thing we must do is find the best possible way to tokenize the entire corpus using our **current** vocabulary. This is what `_dp_decode` does.\n",
    "\n",
    "The function treats tokenization as finding the \"shortest path\" through the text. Imagine the text as a graph where each character is a node. A potential token is an arc or \"road\" connecting the character node at its start to the node at its end. Each road has a toll, which is its cost. The goal is to find the sequence of roads from the beginning to the end with the lowest total toll.\n",
    "\n",
    "This is a classic problem solved efficiently using **Dynamic Programming**, specifically the **Viterbi algorithm**.\n",
    "\n",
    "**The Logic and the Math**\n",
    "\n",
    "The algorithm builds a table, `dp`, where $dp[t,k]$ stores the minimum possible cost to segment the text up to character position $t$, with the very last token belonging to class $k$ (e.g., \"lower\", \"InitCap\", etc.).\n",
    "\n",
    "It calculates this by iterating through each position $t$ in the text and looking backward for all possible split points $i$. The core recurrence relation is:\n",
    "$$dp[t,j]=\\min_{i,k}(dp[i,k]+\\text{cost}(\\text{text}[i:t]))$$\n",
    "\n",
    "Where:\n",
    "* `text[i:t]` is the new token candidate.\n",
    "* $j$ is the class of this new token.\n",
    "* We find the minimum cost by trying all possible previous split points $i$ and all previous token classes $k$.\n",
    "* The `cost` term is a combination of the token's base statistical cost (`_base_token_cost`) and all other linguistic feature costs (`additive_cost`).\n",
    "\n",
    "The most important output of this step is the array `dp_min`. This array, `all_dp_min` in the `train` function, stores the minimum cost to reach **every character position** in the text. $dp_{\\text{min}}[t]$ is the cost of the optimal tokenization from the start of the string up to character $t$. This array is the crucial input for the next phase.\n",
    "\n",
    "---\n",
    "\n",
    "**Phase 2: The Pricing Step - Finding Shortcuts with `_find_best_new_tokens_batch` üí∞**\n",
    "\n",
    "Now that we know the cost of the \"best path\" using our current roads (vocabulary), we can search for potential shortcuts. This is the \"pricing\" or \"column generation\" step. We evaluate every potential token that is **not** currently in our vocabulary and calculate its **Reduced Cost**.\n",
    "\n",
    "The Reduced Cost is the \"profitability\" of adding a new token. It answers the question: \"By how much would the total tokenization cost decrease if we added this new token to our vocabulary?\"\n",
    "\n",
    "**The Logic and the Math**\n",
    "\n",
    "For a candidate token tok that spans from start to end in the text, we can calculate the potential savings.\n",
    "\n",
    "1. **Cost of the Old Path:** The cost to get from the beginning to position `end` using the current vocabulary is already known: it's $dp_{\\text{min}}[end]$.\n",
    "2. **Cost of the New Path:** If we were allowed to use our new token `tok`, the cost to get to `end` would be the cost of the best path to `start`, plus the cost of our new token itself. This is: $dp_{\\text{min}}[\\text{start}]+\\text{cost}(\\text{tok})$.\n",
    "\n",
    "The **Reduced Cost (RC)** is the difference between these two:\n",
    "$$\\text{RC}(\\text{tok})=(dp_{\\text{min}}[\\text{start}]+\\text{cost}(\\text{tok}))‚àídp_{\\text{min}}[\\text{end}]$$\n",
    "\n",
    "If the Reduced Cost is **negative**, it means the new path is cheaper than the old path. This token is a valuable \"shortcut\" that reduces the total segmentation cost.\n",
    "\n",
    "The `_find_best_new_tokens_batch` function iterates through every occurrence of every potential token, calculates its RC, and sums up the total potential savings for each unique token across the entire corpus. It then sorts them and returns the `top_k` tokens with the most negative total RC‚Äîthese are the most valuable new \"expressways\" to build.\n",
    "\n",
    "---\n",
    "\n",
    "**Putting It Together: The Loop**\n",
    "\n",
    "The `train` function simply repeats these two steps:\n",
    "1. `_dp_decode` **(Primal):** Calculate the optimal paths and costs (`dp_min`) for the whole corpus with the current vocabulary.\n",
    "2. `_find_best_new_tokens_batch` **(Pricing):** Use the `dp_min` costs to find the new tokens with the most negative Reduced Cost.\n",
    "3. **Update:** Add these new, high-value tokens to the vocabulary.\n",
    "4. **Repeat:** Continue this cycle until no new tokens with a significantly negative RC can be found, at which point the vocabulary has converged to an optimal state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b69a8-3632-4ad2-aadc-d74c29cf3151",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, lets get the vocabulary! Run the code below, should take ~10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555464e3-7472-4ef0-a280-41b73b6532d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "paragraphs_texts=corpus_texts; paragraphs_langs=corpus_langs; max_iterations=300; rc_stop_tol=-1e-6; verbose=True;\n",
    "\n",
    "if verbose: print(\"\\nStep 2: Starting training with batch pricing...\")\n",
    "for it in range(1, max_iterations + 1):\n",
    "    tokenizer._cost_cache.clear()\n",
    "    # Primal step: Decode the whole corpus with the current vocabulary.\n",
    "    all_dp_min = []\n",
    "    for pi in range(len(paragraphs_texts)):\n",
    "        _, _, dpmin = tokenizer._dp_decode(pi, decode_only=False)\n",
    "        all_dp_min.append(dpmin)\n",
    "\n",
    "    # Pricing step: Find the best new tokens to add.\n",
    "    new_tokens, topk_info = tokenizer._find_best_new_tokens_batch(all_dp_min, top_k=tokenizer.top_k_add)\n",
    "    if not new_tokens:\n",
    "        min_rc = topk_info[0][0] if topk_info else 0.0\n",
    "        if verbose:\n",
    "            print(f\"\\nConvergence: no negative reduced-cost tokens (min summed RC = {min_rc:.6f}).\")\n",
    "        break\n",
    "    min_rc = topk_info[0][0] if topk_info else 0.0\n",
    "\n",
    "    # Check for convergence.\n",
    "    if min_rc >= rc_stop_tol:\n",
    "        if verbose:\n",
    "            print(f\"\\nConvergence: min summed RC = {min_rc:.6f} >= tol {rc_stop_tol:.1e}.\")\n",
    "        break\n",
    "\n",
    "    # Add the new tokens to the vocabulary.\n",
    "    for tok in new_tokens:\n",
    "        tok = ud.normalize(\"NFC\", tok)\n",
    "        if tok not in tokenizer.tok2id:\n",
    "            tokenizer.tok2id[tok] = len(tokenizer.vocab)\n",
    "            tokenizer.vocab.append(tok)\n",
    "\n",
    "    if verbose:\n",
    "        preview = \", \".join([f\"'{t}'\" for t in new_tokens[:6]])\n",
    "        more = \"\" if len(new_tokens) <= 6 else f\" ... (+{len(new_tokens)-6})\"\n",
    "        print(f\"Iter {it:02d}: Added {len(new_tokens)} tokens: {preview}{more} (best summed RC={min_rc:.4f})\")\n",
    "else:\n",
    "    if verbose: print(\"\\nMax iterations reached.\")\n",
    "\n",
    "# If a vocab budget is set, enforce it now.\n",
    "if tokenizer.vocab_budget is not None:\n",
    "    tokenizer._enforce_vocab_budget_bisection(paragraphs_texts, tokenizer.vocab_budget, verbose=verbose)\n",
    "    # Prune exported vocab to active multi-char tokens (plus alphabet)\n",
    "    tokenizer._prune_vocab_to_active(paragraphs_texts)\n",
    "\n",
    "if verbose:\n",
    "    print(f\"\\nTraining complete. Final vocabulary size: {len(tokenizer.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b4a03-6c38-494b-8f0a-5260a37a3349",
   "metadata": {},
   "source": [
    "## ScalableTokenizer.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b03a62-c8e7-451a-98ff-12ada2d2e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lang_codes)):\n",
    "    lang = corpus_langs[i*num_paragraphs]\n",
    "    print(f\"Language: {lang}\")\n",
    "    for j in range(10):\n",
    "        txt = corpus_texts[j+i*num_paragraphs]\n",
    "        tokenized = tokenizer.tokenize(txt, lang)\n",
    "        print(f\"\\t tokenized: {tokenized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf1809-d147-44c8-a2ba-499e9291fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab[-500:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
