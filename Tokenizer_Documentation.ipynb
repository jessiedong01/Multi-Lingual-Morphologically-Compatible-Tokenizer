{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a55dba2bf8674b21b7ac0dd6d5c4bb5d",
      "4b1c275d3682489fb538b22af3376cf6",
      "fba363155c434259b2f9b8be7d24842a",
      "88ef3a3301e147c1bd4bd2cfd8c611ef",
      "0eb685d49e2945f7919ba52105bfd21d",
      "6768fe84a42a454a9e00a93b591f74fa",
      "2ffc8f40361d4c45b18a365af5d6abc1",
      "5e2c45c0879a43b2a903f06950c27bb9",
      "b4816de81fc047b9af36b8a57b0d5b1c",
      "94f4fd81410c43ef85be9cc7ae861f87",
      "55dab127876346babb15dacad9cefed5"
     ]
    },
    "executionInfo": {
     "elapsed": 3279375,
     "status": "ok",
     "timestamp": 1757282904667,
     "user": {
      "displayName": "Anupama Sridhar",
      "userId": "07936693503576571700"
     },
     "user_tz": 420
    },
    "id": "OilnKHViBn8Y",
    "outputId": "27bdaa04-03fb-48ab-8ecc-c8ef936dd62a"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "\n",
    "1. **Core Components & Heuristics**\n",
    "   - Regexes & Constants\n",
    "   - Data Cleaning & Filtering\n",
    "   - Unicode & Script Handling\n",
    "\n",
    "2. **Class Deep Dive**\n",
    "   - `MorphologyEncoder`: The Linguistic Model\n",
    "   - `LinguisticModels`: The Feature Engineering Hub\n",
    "   - `ParagraphInfo`: The Efficiency Cache\n",
    "   - `ScalableTokenizer`: The Main Orchestrator\n",
    "\n",
    "3. **How to Tune and Use**\n",
    "   - The Training Workflow\n",
    "   - Key Hyperparameter Tuning Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Core Components & Heuristics\n",
    "---\n",
    "These functions and constants handle data preprocessing, cleaning, and unicode normalization.\n",
    "\n",
    "### Regexes & Constants (`URL_RE`, `EMAIL_RE`, `NUM_RE`)\n",
    "- **What they do:** These regexes identify common patterns that should *never* be split. They define the \"protected spans\" of text.\n",
    "- **How they work:** The `find_protected_spans` function uses these to find all occurrences and merges any overlapping matches. During DP decoding, these spans are treated as unbreakable, \"atomic\" tokens.\n",
    "- **How to tune:** You can add new regexes to this section to protect other domain-specific patterns (e.g., phone numbers, chemical formulas).\n",
    "\n",
    "### Data Cleaning & Filtering\n",
    "- **`looks_like_redirect`:** Filters out boilerplate Wikipedia redirect pages. Crucial for cleaning web-scraped data. It handles multiple languages and even spaced-out text like \"R E D I R E C T\".\n",
    "- **`WIKI_NOISE_RE`, `QUOTE_RUN_EDGE_RE`:** Filter out common markdown and wiki syntax noise (e.g., `''`, `==`, `**`).\n",
    "- **`clean_junk_runs`:** A post-processing step that collapses repetitive junk tokens found during decoding (e.g., `['.', '.']` -> `['.']`).\n",
    "- **`merge_cjk_runs`:** A post-processing step that merges adjacent single or bi-character CJK (Chinese/Japanese/Korean) tokens into more meaningful words. This is vital for logographic languages.\n",
    "\n",
    "### Unicode & Script Handling\n",
    "- **`is_mark`:** Checks if a character is a non-spacing mark (e.g., an accent).\n",
    "- **`default_allowed_boundaries`:** Prevents splits within complex graphemes (like emojis with skin tones) by disallowing splits next to characters like Zero-Width Joiners (ZWJ).\n",
    "- **`script_guess`:** Provides a quick guess of the primary script/language of a token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Class Deep Dive\n",
    "---\n",
    "\n",
    "### `MorphologyEncoder`\n",
    "- **Purpose:** This class scores how \"well-formed\" a token is for a given language. It helps the tokenizer learn meaningful subwords (like \"running\", \"establishment\") instead of nonsensical fragments.\n",
    "\n",
    "- **How it works:**\n",
    "  1. **Featurization (`_featurize`):** For each token, it extracts character n-grams (e.g., \"ing\", \"run\", \"nn\") and affix features (e.g., starts with \"re-\", ends with \"-tion\").\n",
    "  2. **Embedding (`fit`):** It builds a large matrix of tokens vs. features, computes the Positive Pointwise Mutual Information (PPMI), and then uses SVD (a dimensionality reduction technique) to create a dense vector embedding for every potential token. It also computes a \"prototype\" vector for each language.\n",
    "  3. **Scoring (`score`):** The final score for a (token, language) pair is the cosine similarity between the token's vector and the language's prototype vector. A high score means the token's features are highly characteristic of that language.\n",
    "\n",
    "- **How to tune:**\n",
    "  - **`AFFIXES` / `CROSS_EQUIV`:** You can add prefixes, suffixes, and cross-lingual morphological equivalents for new languages to improve performance.\n",
    "  - **`k`:** The dimensionality of the token embeddings. Higher `k` can capture more nuance but is computationally more expensive.\n",
    "\n",
    "### `LinguisticModels`\n",
    "- **Purpose:** This class bundles all the non-statistical, feature-based costs that are applied during DP decoding. It's where you inject domain knowledge.\n",
    "\n",
    "- **How it works:** The `additive_cost` method calculates a cost based on a variety of features. This cost is *added* to the base statistical cost of a token.\n",
    "\n",
    "- **How to tune:**\n",
    "  - **`lexicon`, `mwe`, `ne_gaz`:** Provide your own dictionaries. Add domain-specific terms, multi-word expressions (\"New York\"), or named entities to encourage the tokenizer to keep them whole.\n",
    "  - **`token_bigram`:** This models the cost of transitioning between *classes* of tokens (e.g., from an \"InitCap\" word to a \"lower\" case word). You can add rules here to encourage or penalize certain grammatical patterns.\n",
    "  - **`gamma_boundary` (float):** **Key parameter.** The penalty for changing token classes. A higher `gamma` encourages longer runs of the same token type (e.g., `['San', 'Jose']` instead of `['San', 'J', 'ose']`).\n",
    "  - **`mu_morph` (float):** **Key parameter.** The weight of the `MorphologyEncoder` score. A higher `mu` makes the tokenizer prioritize morphologically sound tokens.\n",
    "  - **`rho_group` (float):** **Key parameter.** A bias that makes tokens with common affixes slightly \"cheaper,\" encouraging the model to learn them.\n",
    "\n",
    "### `ScalableTokenizer`\n",
    "- **Purpose:** This is the main class that manages the entire training pipeline, from initial data analysis to final vocabulary pruning and tokenization.\n",
    "\n",
    "- **Core Training Loop (`train`):**\n",
    "  1. **`_initialize_stats_and_vocab`:** Scans the entire corpus to find all possible substrings (up to `max_token_len`) and calculates their initial statistical scores (`_nll`, `_pmi_pen`).\n",
    "  2. **Iterative Token Addition:** The loop begins. In each iteration:\n",
    "     a. **`_dp_decode`:** The entire corpus is segmented using the current vocabulary.\n",
    "     b. **`_find_best_new_tokens_batch`:** It calculates the \"reduced cost\" for all *potential* tokens not yet in the vocabulary. A negative reduced cost means adding that token would improve the overall segmentation quality.\n",
    "     c. **Add Tokens:** The `top_k_add` best tokens are added to the vocabulary.\n",
    "  3. **Loop Termination:** The loop stops when no more tokens have a negative reduced cost or `max_iterations` is reached.\n",
    "\n",
    "- **Vocabulary Budgeting (`_enforce_vocab_budget_bisection`):**\n",
    "  - **Purpose:** This is the key step to precisely hit the `vocab_budget`.\n",
    "  - **How it works:** It performs a bisection search to find the optimal Lagrangian multiplier (`_lambda_global`). This `λ` is a global cost added to every multi-character token. A higher `λ` makes long tokens more \"expensive,\" forcing the DP decoder to use fewer unique types, thus reducing the vocabulary size.\n",
    "\n",
    "- **Tokenization (`tokenize`):**\n",
    "  - **Purpose:** The final inference method to tokenize new text.\n",
    "  - **How it works:** It runs the same `_dp_decode` algorithm on the input text using the final, trained vocabulary and cost models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. How to Tune and Use\n",
    "---\n",
    "\n",
    "### The Training Workflow\n",
    "1.  **Prepare Data:** Provide a list of strings (`paragraphs_texts`) and corresponding language codes (`paragraphs_langs`).\n",
    "2.  **Set Linguistic Features (Optional):** Instantiate the tokenizer and call `set_feature_models()` with your custom lexicons, bigram costs, and weights (`mu_morph`, `gamma_boundary`, etc.).\n",
    "3.  **Train:** Call the `tokenizer.train(...)` method.\n",
    "4.  **Tokenize:** Use the trained `tokenizer.tokenize(text)` method.\n",
    "\n",
    "### Key Hyperparameter Tuning Guide\n",
    "- **To control the statistical base cost:**\n",
    "  - **`alpha`:** Weight for Negative Log-Likelihood (frequency). Higher `alpha` favors more frequent tokens.\n",
    "  - **`beta`:** Weight for PMI-like cohesion score. Higher `beta` favors tokens that are more cohesive than their characters would suggest (e.g., \"ing\" is cohesive, \"qjx\" is not).\n",
    "  - **`tau`:** Per-character length penalty. Higher `tau` favors shorter tokens.\n",
    "\n",
    "- **To control the linguistic intelligence:**\n",
    "  - **`mu_morph`:** How much to trust the morphology model. Increase this if you get lots of nonsensical subwords.\n",
    "  - **`gamma_boundary`:** How much to penalize switching token types. Increase this if you want to keep runs of capitalized words or numbers together.\n",
    "\n",
    "- **To control the vocabulary:**\n",
    "  - **`min_freq`:** The frequency gate for a substring to even be considered. Higher values lead to a smaller initial candidate pool and faster training.\n",
    "  - **`vocab_budget`:** The final target size for your multi-character vocabulary.\n",
    "  - **`top_k_add`:** The number of new tokens added per training iteration. A smaller value leads to slower but potentially more stable convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from Hugging Face datasets hub...\n",
      "-> Loading 'English' (en)...\n",
      "-> Loading 'German' (de)...\n",
      "-> Loading 'French' (fr)...\n",
      "-> Loading 'Turkish' (tr)...\n",
      "-> Loading 'Russian' (ru)...\n",
      "-> Loading 'Japanese' (ja)...\n",
      "-> Loading 'Arabic' (ar)...\n",
      "-> Loading 'Tamil' (ta)...\n",
      "-> Loading 'Xhosa' (xh)...\n",
      "Could not load data for xh: BuilderConfig 'xh' not found. Available: ['ace', 'af', 'als', 'am', 'an', 'ang', 'ar', 'arc', 'arz', 'as', 'ast', 'ay', 'az', 'ba', 'bar', 'bat-smg', 'be', 'be-x-old', 'bg', 'bh', 'bn', 'bo', 'br', 'bs', 'ca', 'cbk-zam', 'cdo', 'ce', 'ceb', 'ckb', 'co', 'crh', 'cs', 'csb', 'cv', 'cy', 'da', 'de', 'diq', 'dv', 'el', 'eml', 'en', 'eo', 'es', 'et', 'eu', 'ext', 'fa', 'fi', 'fiu-vro', 'fo', 'fr', 'frr', 'fur', 'fy', 'ga', 'gan', 'gd', 'gl', 'gn', 'gu', 'hak', 'he', 'hi', 'hr', 'hsb', 'hu', 'hy', 'ia', 'id', 'ig', 'ilo', 'io', 'is', 'it', 'ja', 'jbo', 'jv', 'ka', 'kk', 'km', 'kn', 'ko', 'ksh', 'ku', 'ky', 'la', 'lb', 'li', 'lij', 'lmo', 'ln', 'lt', 'lv', 'map-bms', 'mg', 'mhr', 'mi', 'min', 'mk', 'ml', 'mn', 'mr', 'ms', 'mt', 'mwl', 'my', 'mzn', 'nap', 'nds', 'ne', 'nl', 'nn', 'no', 'nov', 'oc', 'or', 'os', 'pa', 'pdc', 'pl', 'pms', 'pnb', 'ps', 'pt', 'qu', 'rm', 'ro', 'ru', 'rw', 'sa', 'sah', 'scn', 'sco', 'sd', 'sh', 'si', 'simple', 'sk', 'sl', 'so', 'sq', 'sr', 'su', 'sv', 'sw', 'szl', 'ta', 'te', 'tg', 'th', 'tk', 'tl', 'tr', 'tt', 'ug', 'uk', 'ur', 'uz', 'vec', 'vep', 'vi', 'vls', 'vo', 'wa', 'war', 'wuu', 'xmf', 'yi', 'yo', 'zea', 'zh', 'zh-classical', 'zh-min-nan', 'zh-yue']\n",
      "-> Loading 'Zulu' (zu)...\n",
      "Could not load data for zu: BuilderConfig 'zu' not found. Available: ['ace', 'af', 'als', 'am', 'an', 'ang', 'ar', 'arc', 'arz', 'as', 'ast', 'ay', 'az', 'ba', 'bar', 'bat-smg', 'be', 'be-x-old', 'bg', 'bh', 'bn', 'bo', 'br', 'bs', 'ca', 'cbk-zam', 'cdo', 'ce', 'ceb', 'ckb', 'co', 'crh', 'cs', 'csb', 'cv', 'cy', 'da', 'de', 'diq', 'dv', 'el', 'eml', 'en', 'eo', 'es', 'et', 'eu', 'ext', 'fa', 'fi', 'fiu-vro', 'fo', 'fr', 'frr', 'fur', 'fy', 'ga', 'gan', 'gd', 'gl', 'gn', 'gu', 'hak', 'he', 'hi', 'hr', 'hsb', 'hu', 'hy', 'ia', 'id', 'ig', 'ilo', 'io', 'is', 'it', 'ja', 'jbo', 'jv', 'ka', 'kk', 'km', 'kn', 'ko', 'ksh', 'ku', 'ky', 'la', 'lb', 'li', 'lij', 'lmo', 'ln', 'lt', 'lv', 'map-bms', 'mg', 'mhr', 'mi', 'min', 'mk', 'ml', 'mn', 'mr', 'ms', 'mt', 'mwl', 'my', 'mzn', 'nap', 'nds', 'ne', 'nl', 'nn', 'no', 'nov', 'oc', 'or', 'os', 'pa', 'pdc', 'pl', 'pms', 'pnb', 'ps', 'pt', 'qu', 'rm', 'ro', 'ru', 'rw', 'sa', 'sah', 'scn', 'sco', 'sd', 'sh', 'si', 'simple', 'sk', 'sl', 'so', 'sq', 'sr', 'su', 'sv', 'sw', 'szl', 'ta', 'te', 'tg', 'th', 'tk', 'tl', 'tr', 'tt', 'ug', 'uk', 'ur', 'uz', 'vec', 'vep', 'vi', 'vls', 'vo', 'wa', 'war', 'wuu', 'xmf', 'yi', 'yo', 'zea', 'zh', 'zh-classical', 'zh-min-nan', 'zh-yue']\n",
      "-> Loading 'Turkmen' (tk)...\n",
      "Corpus loading complete.\n",
      "------------------------------------------------------------\n",
      "Step 1: Performing initial corpus analysis...\n",
      "Analysis complete in 3.89s. Found 16006 potential tokens; seed vocab chars = 1585.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m   -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     23\u001b[39m tb  = {\n\u001b[32m     24\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33m<BOS>\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mInitCap\u001b[39m\u001b[33m\"\u001b[39m): -\u001b[32m0.2\u001b[39m,\n\u001b[32m     25\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mInitCap\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mInitCap\u001b[39m\u001b[33m\"\u001b[39m): -\u001b[32m0.3\u001b[39m,\n\u001b[32m     26\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mNUM\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNUM\u001b[39m\u001b[33m\"\u001b[39m): -\u001b[32m0.15\u001b[39m,\n\u001b[32m     27\u001b[39m }\n\u001b[32m     28\u001b[39m tokenizer.set_feature_models(\n\u001b[32m     29\u001b[39m     lexicon=lex,\n\u001b[32m     30\u001b[39m     ne_gaz=ne,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     rho_group=\u001b[32m0.06\u001b[39m\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m tokenizer.train(corpus_texts, corpus_langs, max_iterations=\u001b[32m300\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Tokenization Examples ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m tests = [\n\u001b[32m     41\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mThis is a final test of the representations.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     42\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mDie endgültige Prüfung der Darstellungen.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mde\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33m# Yönlendirme Türkiye\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtr\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     50\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/teachings/spring_cs197/tokenizer.py:250\u001b[39m, in \u001b[36mScalableTokenizer.train\u001b[39m\u001b[34m(self, paragraphs_texts, paragraphs_langs, max_iterations, rc_stop_tol, verbose)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, paragraphs_texts, paragraphs_langs, max_iterations=\u001b[32m1000\u001b[39m, rc_stop_tol=-\u001b[32m1e-6\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28mself\u001b[39m._initialize_stats_and_vocab(paragraphs_texts, paragraphs_langs)\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStep 2: Starting training with batch pricing...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_iterations + \u001b[32m1\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/teachings/spring_cs197/tokenizer.py:126\u001b[39m, in \u001b[36mScalableTokenizer._initialize_stats_and_vocab\u001b[39m\u001b[34m(self, paragraphs_texts, paragraphs_langs)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28mself\u001b[39m._ling.morph_encoder = MorphologyEncoder()\n\u001b[32m    125\u001b[39m \u001b[38;5;28mself\u001b[39m._ling.paragraph_lang = \u001b[38;5;28mself\u001b[39m.paragraph_lang\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28mself\u001b[39m._ling.morph_encoder.fit(\u001b[38;5;28mself\u001b[39m._paras, \u001b[38;5;28mself\u001b[39m._token_occurrences, \u001b[38;5;28mself\u001b[39m.paragraph_lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/teachings/spring_cs197/linguistic_features.py:86\u001b[39m, in \u001b[36mMorphologyEncoder.fit\u001b[39m\u001b[34m(self, paragraphs, tok_occurrences, paragraph_lang)\u001b[39m\n\u001b[32m     84\u001b[39m G = PPMI @ PPMI.T\n\u001b[32m     85\u001b[39m G = (G + G.T) * \u001b[32m0.5\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m eigvals, eigvecs = np.linalg.eigh(G)\n\u001b[32m     87\u001b[39m idx = np.argsort(eigvals)[::-\u001b[32m1\u001b[39m][:\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.k, G.shape[\u001b[32m0\u001b[39m])]\n\u001b[32m     88\u001b[39m V = eigvecs[:, idx] * np.sqrt(np.maximum(eigvals[idx], \u001b[32m0\u001b[39m))[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/numpy/linalg/_linalg.py:1677\u001b[39m, in \u001b[36meigh\u001b[39m\u001b[34m(a, UPLO)\u001b[39m\n\u001b[32m   1673\u001b[39m signature = \u001b[33m'\u001b[39m\u001b[33mD->dD\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33md->dd\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1674\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,\n\u001b[32m   1675\u001b[39m               invalid=\u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m, over=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, divide=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1676\u001b[39m               under=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1677\u001b[39m     w, vt = gufunc(a, signature=signature)\n\u001b[32m   1678\u001b[39m w = w.astype(_realType(result_t), copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1679\u001b[39m vt = vt.astype(result_t, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    lang_codes = {\n",
    "        'en': 'English', 'da': 'Danish',\n",
    "        #'de': 'German', 'fr': 'French',\n",
    "        #'tr': 'Turkish', 'ru': 'Russian', 'ja': 'Japanese',\n",
    "        #'ar': 'Arabic', 'ta': 'Tamil', 'xh': 'Xhosa',\n",
    "        #'zu': 'Zulu', 'tk': 'Turkmen'\n",
    "    }\n",
    "\n",
    "    corpus_texts, corpus_langs = load_wikiann_corpus(lang_codes, per_lang=700)\n",
    "    if not corpus_texts:\n",
    "        return\n",
    "\n",
    "    tokenizer = ScalableTokenizer(\n",
    "        max_token_len=12, min_freq=7, top_k_add=8, vocab_budget=500\n",
    "    )\n",
    "\n",
    "    # Morphology-aware + TV knobs\n",
    "    lex = {\"New York\": 2.0, \"San Jose\": 1.0, \"’s\": 0.5, \"'s\": 0.5}\n",
    "    ne  = {\"LOC\": {\"New York\", \"Berlin\", \"東京\"}}\n",
    "    tb  = {\n",
    "        (\"<BOS>\", \"InitCap\"): -0.2,\n",
    "        (\"InitCap\", \"InitCap\"): -0.3,\n",
    "        (\"NUM\", \"NUM\"): -0.15,\n",
    "    }\n",
    "    tokenizer.set_feature_models(\n",
    "        lexicon=lex,\n",
    "        ne_gaz=ne,\n",
    "        token_bigram=tb,\n",
    "        gamma_boundary=0.06,\n",
    "        mu_morph=0.25,\n",
    "        rho_group=0.06\n",
    "    )\n",
    "\n",
    "    tokenizer.train(corpus_texts, corpus_langs, max_iterations=300)\n",
    "\n",
    "    print(\"\\n--- Tokenization Examples ---\")\n",
    "    tests = [\n",
    "        (\"This is a final test of the representations.\", \"en\"),\n",
    "        (\"Die endgültige Prüfung der Darstellungen.\", \"de\"),\n",
    "        (\"Temsilleriň soňky synagy.\", \"tk\"),\n",
    "        (\"表現の最終テストです。\", \"ja\"),\n",
    "        (\"Email me at alice@example.com or visit https://example.org/docs.\", \"en\"),\n",
    "        (\"The price was 12,345.67 dollars on 2024-09-04.\", \"en\"),\n",
    "        (\"#REDIRECT United States\", \"en\"),\n",
    "        (\"# Weiterleitung Berlin\", \"de\"),\n",
    "        (\"# Yönlendirme Türkiye\", \"tr\"),\n",
    "    ]\n",
    "    for sentence, lang in tests:\n",
    "        tokens = tokenizer.tokenize(sentence, lang=lang)\n",
    "        print(f\"   '{sentence}'\\n   -> {tokens}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNZwNMk2PnanWFNNBZBzvYq",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0eb685d49e2945f7919ba52105bfd21d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ffc8f40361d4c45b18a365af5d6abc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b1c275d3682489fb538b22af3376cf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6768fe84a42a454a9e00a93b591f74fa",
      "placeholder": "​",
      "style": "IPY_MODEL_2ffc8f40361d4c45b18a365af5d6abc1",
      "value": "README.md: "
     }
    },
    "55dab127876346babb15dacad9cefed5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e2c45c0879a43b2a903f06950c27bb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "6768fe84a42a454a9e00a93b591f74fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88ef3a3301e147c1bd4bd2cfd8c611ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94f4fd81410c43ef85be9cc7ae861f87",
      "placeholder": "​",
      "style": "IPY_MODEL_55dab127876346babb15dacad9cefed5",
      "value": " 158k/? [00:00&lt;00:00, 15.4MB/s]"
     }
    },
    "94f4fd81410c43ef85be9cc7ae861f87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a55dba2bf8674b21b7ac0dd6d5c4bb5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4b1c275d3682489fb538b22af3376cf6",
       "IPY_MODEL_fba363155c434259b2f9b8be7d24842a",
       "IPY_MODEL_88ef3a3301e147c1bd4bd2cfd8c611ef"
      ],
      "layout": "IPY_MODEL_0eb685d49e2945f7919ba52105bfd21d"
     }
    },
    "b4816de81fc047b9af36b8a57b0d5b1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fba363155c434259b2f9b8be7d24842a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e2c45c0879a43b2a903f06950c27bb9",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b4816de81fc047b9af36b8a57b0d5b1c",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
