{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870b606c-8a30-49b2-bbaa-a13bb9230880",
   "metadata": {},
   "source": [
    "# Exploring the Multilingual Tokenizer üßê\n",
    "*(contact: arjo@stanford.edu)*\n",
    "\n",
    "This notebook walks through some of the core elements of the scalable, linguistically-aware tokenizer. \n",
    "\n",
    "## 1. The Building Blocks:\n",
    "`utils.py` and `constants.py`\n",
    "\n",
    "The tokenizer relies on several helper functions for text pre-processing, Unicode handling, and noise filtering. Let's see them in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f00236b9-026c-46d8-9d6d-f83633b922dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: 'Contact me at sasha@example.com, see [https://example.org](https://example.org), or pay $1,234.56.'\n",
      "Protected Spans (start, end): [(14, 31), (38, 80), (89, 97)]\n",
      " -> Found atomic unit: 'sasha@example.com'\n",
      " -> Found atomic unit: 'https://example.org](https://example.org),'\n",
      " -> Found atomic unit: '1,234.56'\n"
     ]
    }
   ],
   "source": [
    "# Import the modules\n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "# Let's start with \"protected spans\". The tokenizer identifies things like URLs,\n",
    "# emails, and complex numbers that should *never* be split.\n",
    "test_text_1 = \"Contact me at sasha@example.com, see [https://example.org](https://example.org), or pay $1,234.56.\"\n",
    "protected = utils.find_protected_spans(test_text_1)\n",
    "\n",
    "print(f\"Original text: '{test_text_1}'\")\n",
    "print(\"Protected Spans (start, end):\", protected)\n",
    "for start, end in protected:\n",
    "    print(f\" -> Found atomic unit: '{test_text_1[start:end]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f98483-dfd1-4bf4-ab06-eb928bebba89",
   "metadata": {},
   "source": [
    "---\n",
    "A key feature is its awareness of **Unicode graphemes**. It knows not to split a character from its accent or an emoji from its modifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "701efff4-5552-40d9-84c6-0b8876539d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: üë©‚Äçüíª is an √©moji.\n",
      "Allowed Boundaries (1=OK, 0=NO):\n",
      "|üë©X‚ÄçXüíª| |i|s| |a|n| |√©|m|o|j|i|.|\n",
      "\n",
      "Notice it correctly prevents splits (X) inside the emoji and the accented '√©'.\n"
     ]
    }
   ],
   "source": [
    "# An emoji with a skin tone modifier (üë© + ZWJ + üíª = woman technologist)\n",
    "# and a character with a combining accent mark ('e' + ¬¥)\n",
    "test_text_2 = \"üë©‚Äçüíª is an √©moji.\"\n",
    "\n",
    "# The `default_allowed_boundaries` function creates a boolean mask.\n",
    "# `True` means a split is allowed BEFORE that character index.\n",
    "boundaries = utils.default_allowed_boundaries(test_text_2)\n",
    "\n",
    "print(f\"Text: {test_text_2}\")\n",
    "print(\"Allowed Boundaries (1=OK, 0=NO):\")\n",
    "\n",
    "# Let's visualize where it avoids splitting\n",
    "viz = \"\"\n",
    "for i, char in enumerate(test_text_2):\n",
    "    split_marker = \"|\" if boundaries[i] else \"X\"\n",
    "    viz += f\"{split_marker}{char}\"\n",
    "viz += \"|\" if boundaries[len(test_text_2)] else \"X\"\n",
    "print(viz)\n",
    "print(\"\\nNotice it correctly prevents splits (X) inside the emoji and the accented '√©'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b6ce4-a522-43e9-afb6-2a27ce89cb94",
   "metadata": {},
   "source": [
    "---\n",
    "Finally, it includes special post-processing for **CJK (Chinese, Japanese, Korean)** languages, where words are often not space-separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c693be-8d46-48d0-bdea-4d2953ae3351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial (over-eager) split: ['„Åì„Çå„ÅØ', 'Êó•', 'Êú¨', 'Ë™û', '„Åß„Åô', '„ÄÇ']\n",
      "After CJK merging rule:    ['„Åì„Çå„ÅØ', 'Êó•Êú¨Ë™û„Åß„Åô', '„ÄÇ']\n"
     ]
    }
   ],
   "source": [
    "# Imagine the tokenizer initially splits a Japanese word into characters.\n",
    "initial_split = ['„Åì„Çå„ÅØ', 'Êó•', 'Êú¨', 'Ë™û', '„Åß„Åô', '„ÄÇ']\n",
    "merged_split = utils.merge_cjk_runs(initial_split)\n",
    "\n",
    "print(f\"Initial (over-eager) split: {initial_split}\")\n",
    "print(f\"After CJK merging rule:    {merged_split}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1872927-cb01-4374-8989-ebd78ce7e589",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e94d1-508c-4c1b-a517-b6468d8985bd",
   "metadata": {},
   "source": [
    "## Try yourself ‚≠ê\n",
    "Explore the rest of the functionality in `utils.py`, in particular: `looks_like_redirect`, `clean_junk_runs`, `script_guess`, `_is_mixed_script`, and `ParagraphInfo`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238eb56e-f5ff-449b-97bf-ebf472eb4410",
   "metadata": {},
   "source": [
    "## 2. Linguistic Intelligence: linguistic_features.py\n",
    "\n",
    "This tokenizer goes beyond statistics by incorporating linguistic knowledge. This is managed by the `LinguisticModels` and `MorphologyEncoder` classes.\n",
    "\n",
    "**The Morphology Encoder**\n",
    "\n",
    "This is the most sophisticated component. It learns vector representations of words based on their character patterns (n-grams) and known affixes (prefixes/suffixes). This helps it understand that \"running\" and \"jumping\" are similar, even if it has never seen \"jumping\" before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6fa7d7-4e4e-4250-8d52-17263b88622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Morphology Encoder trained on mini-corpus.\n"
     ]
    }
   ],
   "source": [
    "from linguistic_features import MorphologyEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Create a mini-corpus to train the encoder ---\n",
    "# A real corpus would have thousands of paragraphs.\n",
    "mini_corpus_texts = [\n",
    "    \"She is running and jumping.\",\n",
    "    \"He likes walking.\",\n",
    "    \"They are runners.\"\n",
    "]\n",
    "mini_corpus_langs = [\"en\", \"en\", \"en\"]\n",
    "\n",
    "# The encoder needs a map of all potential tokens and where they occur.\n",
    "# We'll simulate this for a few key words.\n",
    "tok_occurrences = defaultdict(list)\n",
    "tok_occurrences[\"running\"].append((0, 9))\n",
    "tok_occurrences[\"jumping\"].append((0, 21))\n",
    "tok_occurrences[\"walking\"].append((1, 9))\n",
    "tok_occurrences[\"runners\"].append((2, 12))\n",
    "tok_occurrences[\"likes\"].append((1, 3))\n",
    "\n",
    "# A helper function to get the language of a paragraph\n",
    "def para_lang(idx):\n",
    "    return mini_corpus_langs[idx]\n",
    "\n",
    "# --- Train the encoder ---\n",
    "# We use k=4 for tiny 4-dimensional vectors for this demo.\n",
    "encoder = MorphologyEncoder(k=4)\n",
    "encoder.fit(mini_corpus_texts, tok_occurrences, para_lang)\n",
    "\n",
    "print(\"‚úÖ Morphology Encoder trained on mini-corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc69806-c7c0-456d-a4d0-89f549c7fa26",
   "metadata": {},
   "source": [
    "Now, let's inspect the results. The encoder has learned a **prototype vector** for English, representing the \"average\" morphological shape of an English word in our tiny corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6c7560-b4ae-440a-a83a-83944c0d619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned 'en' prototype vector (shape (4,)):\n",
      "[ 4.3481302e-01 -7.1796298e-01  5.0539441e-15 -5.4356873e-01]\n",
      "\n",
      "Morphological fit score for 'potato': 0.0000\n",
      "Morphological fit score for 'running': 0.8069\n",
      "Morphological fit score for 'walking': 0.4156\n",
      "Morphological fit score for 'runners': 0.7271\n"
     ]
    }
   ],
   "source": [
    "# Get the prototype vector for English\n",
    "proto_en = encoder.lang_proto.get(\"en\")\n",
    "print(f\"Learned 'en' prototype vector (shape {proto_en.shape}):\\n{proto_en}\\n\")\n",
    "\n",
    "# Let's check the similarity score of our words against this prototype.\n",
    "# The score is the cosine similarity between the word's vector and the language prototype.\n",
    "# A higher score means it's a more \"typical\" word for that language.\n",
    "for word in [\"potato\", \"running\", \"walking\", \"runners\"]:\n",
    "    score = encoder.score(word, \"en\")\n",
    "    print(f\"Morphological fit score for '{word}': {score:.4f}\")\n",
    "\n",
    "# Notice how words with common English suffixes ('ing', 'ers') get high scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a6d70-d8b1-4020-9b30-541f5efb3692",
   "metadata": {},
   "source": [
    "**The Cost Model**\n",
    "\n",
    "The `LinguisticModels` class aggregates all these hints into a single **cost**. The tokenizer's goal is to find the segmentation with the minimum total cost. A negative cost is a **reward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916baa44-a38e-4178-b362-e7a9937f1f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total additive cost for 'Running': -0.2000\n",
      "\n",
      "Let's break down the cost:\n",
      " -> Bigram reward: -0.2000\n",
      " -> Morphology reward (cost): -0.0000\n",
      " -> Affix reward (cost): 0.0500\n"
     ]
    }
   ],
   "source": [
    "from linguistic_features import LinguisticModels\n",
    "\n",
    "# Let's define some linguistic \"hints\"\n",
    "lex = {\"New York\": 2.0} # Encourage \"New York\" to be one token\n",
    "tb  = {(\"<BOS>\", \"InitCap\"): -0.2} # Reward sentences that start with a capital letter\n",
    "\n",
    "# We'll attach our trained morphology encoder\n",
    "models = LinguisticModels(lexicon=lex, token_bigram=tb)\n",
    "models.morph_encoder = encoder\n",
    "models.paragraph_lang = para_lang\n",
    "\n",
    "# --- Calculate the cost for a token in context ---\n",
    "# Context: The token \"Running\" appears at the start of paragraph 0 (which is English).\n",
    "# The previous token class is \"<BOS>\" (Begin Of Sentence).\n",
    "cost = models.additive_cost(token=\"Running\", prev_class=\"<BOS>\", paragraph_idx=0)\n",
    "print(f\"Total additive cost for 'Running': {cost:.4f}\\n\")\n",
    "\n",
    "print(\"Let's break down the cost:\")\n",
    "# 1. Bigram reward: Starts with InitCap after <BOS>\n",
    "print(f\" -> Bigram reward: {tb[('<BOS>', 'InitCap')]:.4f}\")\n",
    "# 2. Morphology reward: The morphology score is high, so the cost is negative\n",
    "morph_cost = -models.mu_morph * encoder.score(\"Running\", \"en\")\n",
    "print(f\" -> Morphology reward (cost): {morph_cost:.4f}\")\n",
    "# 3. Affix reward: has the \"-ing\" suffix\n",
    "affix_cost = models._affix_bias(\"Running\", \"en\")\n",
    "print(f\" -> Affix reward (cost): {affix_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a50c7-8778-4536-8398-3c7d8f8bf54b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ebe410-f962-4528-b843-d2c0d8400e39",
   "metadata": {},
   "source": [
    "**3. The Core Algorithm:** `tokenizer.py`\n",
    "\n",
    "Now we combine everything. The `ScalableTokenizer` class uses a **dynamic programming** algorithm (`_dp_decode`) to find the lowest-cost path through the text.\n",
    "\n",
    "**Training: The Iterative Process**\n",
    "\n",
    "Training is an iterative loop:\n",
    "\n",
    "1. **Decode:** Segment the entire corpus using the current vocabulary.\n",
    "2. **Price:** Find new candidate tokens that would lower the total segmentation cost the most.\n",
    "3. **Update:** Add the best new tokens to the vocabulary.\n",
    "4. Repeat until no more \"good\" tokens can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a06c38c6-b1f9-4fbf-b9f4-4ec60c92b5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Performing initial corpus analysis...\n",
      "Analysis complete in 0.01s. Found 1029 potential tokens; seed vocab chars = 25.\n",
      "\n",
      "Step 2: Starting training with batch pricing...\n",
      "Iter 01: Added 1 tokens: ' tokenizer' (best summed RC=-69.7859)\n",
      "Iter 02: Added 1 tokens: ' tokenize' (best summed RC=-64.1177)\n",
      "Iter 03: Added 1 tokens: ' tokeniz' (best summed RC=-58.5165)\n",
      "Iter 04: Added 1 tokens: 'deutscher Satz' (best summed RC=-50.8783)\n",
      "Iter 05: Added 1 tokens: ' deutscher Sat' (best summed RC=-49.7266)\n",
      "Iter 06: Added 1 tokens: 'n deutscher Sa' (best summed RC=-49.3962)\n",
      "Iter 07: Added 1 tokens: 'in deutscher S' (best summed RC=-48.5323)\n",
      "Iter 08: Added 1 tokens: 'okeni' (best summed RC=-48.4530)\n",
      "Iter 09: Added 1 tokens: 'rung wird gele' (best summed RC=-48.3802)\n",
      "Iter 10: Added 1 tokens: 'erung wird gel' (best summed RC=-48.2789)\n",
      "Iter 11: Added 1 tokens: 'sierung wird g' (best summed RC=-47.9548)\n",
      "Iter 12: Added 1 tokens: 'st ein deutsch' (best summed RC=-47.5049)\n",
      "Iter 13: Added 1 tokens: 'ist ein deutsc' (best summed RC=-46.3016)\n",
      "Iter 14: Added 1 tokens: 'rs are useful.' (best summed RC=-44.6165)\n",
      "Iter 15: Added 1 tokens: ' ist ein deuts' (best summed RC=-44.6097)\n",
      "Iter 16: Added 1 tokens: 'sierung wird ' (best summed RC=-44.3753)\n",
      "Iter 17: Added 1 tokens: ' is learning.' (best summed RC=-43.2225)\n",
      "Iter 18: Added 1 tokens: 'sierung wird' (best summed RC=-41.8969)\n",
      "Iter 19: Added 1 tokens: ' ist ein deut' (best summed RC=-41.7167)\n",
      "Iter 20: Added 1 tokens: 'rs are useful' (best summed RC=-41.1928)\n",
      "Iter 21: Added 1 tokens: ' token' (best summed RC=-40.2328)\n",
      "Iter 22: Added 1 tokens: ' is learning' (best summed RC=-39.8371)\n",
      "Iter 23: Added 1 tokens: 'Das ist ein ' (best summed RC=-39.5208)\n",
      "Iter 24: Added 1 tokens: 'sierung wir' (best summed RC=-38.2609)\n",
      "Iter 25: Added 1 tokens: 'rs are usefu' (best summed RC=-37.6107)\n",
      "Iter 26: Added 1 tokens: 'deutscher Sat' (best summed RC=-37.0506)\n",
      "Iter 27: Added 1 tokens: 'Das ist ein' (best summed RC=-37.0443)\n",
      "Iter 28: Added 1 tokens: ' is learnin' (best summed RC=-36.2131)\n",
      "Iter 29: Added 1 tokens: 'sierung wi' (best summed RC=-35.3652)\n",
      "Iter 30: Added 1 tokens: 'Good toke' (best summed RC=-34.4390)\n",
      "Iter 31: Added 1 tokens: 'izers are usef' (best summed RC=-44.9652)\n",
      "Iter 32: Added 1 tokens: 'nizers are use' (best summed RC=-43.7578)\n",
      "Iter 33: Added 1 tokens: 'nizers are us' (best summed RC=-41.5741)\n",
      "Iter 34: Added 1 tokens: 'nizers are u' (best summed RC=-38.7459)\n",
      "Iter 35: Added 1 tokens: 'nizers are ' (best summed RC=-35.4091)\n",
      "Iter 36: Added 1 tokens: ' deutscher Sa' (best summed RC=-34.2519)\n",
      "Iter 37: Added 1 tokens: 'Das ist ei' (best summed RC=-34.1489)\n",
      "Iter 38: Added 1 tokens: ' is learni' (best summed RC=-33.3747)\n",
      "Iter 39: Added 1 tokens: 'This toke' (best summed RC=-33.2437)\n",
      "Iter 40: Added 1 tokens: 'nizer is learn' (best summed RC=-43.1613)\n",
      "Iter 41: Added 1 tokens: 'nizer is lear' (best summed RC=-40.3235)\n",
      "Iter 42: Added 1 tokens: 'nizer is lea' (best summed RC=-37.4733)\n",
      "Iter 43: Added 1 tokens: 'nizer is le' (best summed RC=-34.0853)\n",
      "Iter 44: Added 1 tokens: 'nizers are' (best summed RC=-32.9404)\n",
      "Iter 45: Added 1 tokens: 'Die Token' (best summed RC=-32.7978)\n",
      "Iter 46: Added 1 tokens: 'isierung w' (best summed RC=-35.4138)\n",
      "Iter 47: Added 1 tokens: 'Good tok' (best summed RC=-32.1599)\n",
      "Iter 48: Added 1 tokens: 'nizer is l' (best summed RC=-31.7958)\n",
      "Iter 49: Added 1 tokens: 'Das ist e' (best summed RC=-31.6447)\n",
      "Iter 50: Added 1 tokens: 'isierung ' (best summed RC=-31.2754)\n",
      "\n",
      "Max iterations reached.\n",
      "\n",
      "Step 3: Enforcing vocabulary budget K=20 via Lagrangian bisection on Œª.\n",
      "  Œª=1.000000 ‚Üí used types = 8 (target 20)\n",
      "  Œª=0.500000 ‚Üí used types = 8 (target 20)\n",
      "  Œª=0.250000 ‚Üí used types = 8 (target 20)\n",
      "  Œª=0.125000 ‚Üí used types = 8 (target 20)\n",
      "  Œª=0.062500 ‚Üí used types = 8 (target 20)\n",
      "  Œª=0.031250 ‚Üí used types = 8 (target 20)\n",
      "  Œª=0.015625 ‚Üí used types = 8 (target 20)\n",
      "  Œª=0.007812 ‚Üí used types = 8 (target 20)\n",
      "Chosen Œª*=1.000000 (gap=12).\n",
      "\n",
      "Training complete. Final vocabulary size: 33\n",
      "['\\n', ' ', '.', 'D', 'G', 'S', 'T', 'a', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'n', 'o', 'r', 's', 't', 'u', 'w', 'z', ' is learning.', ' tokenizer', 'Das ist ein ', 'Die Token', 'Good toke', 'deutscher Satz', 'rs are useful.', 'rung wird gele']\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import ScalableTokenizer\n",
    "\n",
    "# Let's use a slightly larger, multilingual sample corpus for training.\n",
    "corpus_texts = [\n",
    "    \"This tokenizer is learning.\",\n",
    "    \"Good tokenizers are useful.\",\n",
    "    \"Das ist ein deutscher Satz.\",\n",
    "    \"Die Tokenisierung wird gelernt.\"\n",
    "]\n",
    "corpus_langs = [\"en\", \"en\", \"de\", \"de\"]\n",
    "\n",
    "# Initialize the tokenizer.\n",
    "# For this demo, we'll use a very low min_freq and a small vocab_budget.\n",
    "tokenizer = ScalableTokenizer(\n",
    "    min_freq=1,\n",
    "    top_k_add=1,\n",
    "    vocab_budget=20 # Target ~20 multi-character tokens\n",
    ")\n",
    "\n",
    "# Train for just a few iterations to see the process.\n",
    "# In a real scenario, this would run for hundreds of iterations.\n",
    "tokenizer.train(corpus_texts, corpus_langs, max_iterations=50)\n",
    "print(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7684928-d1ba-4201-becf-4ff8d403b269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priming the tokenizer with linguistic hints...\n",
      "\n",
      "Starting the training process...\n",
      "Step 1: Performing initial corpus analysis...\n",
      "Analysis complete in 0.02s. Found 1029 potential tokens; seed vocab chars = 25.\n",
      "\n",
      "Step 2: Starting training with batch pricing...\n",
      "Iter 01: Added 5 tokens: 'tokenizer', ' tokenizer', ' tokenize', ' tokeniz', 'tokenize' (best summed RC=-74.9256)\n",
      "Iter 02: Added 5 tokens: 'Tokenisierung', 'deutscher Satz', 'eutscher Satz.', ' deutscher Sat', 'n deutscher Sa' (best summed RC=-54.8142)\n",
      "Iter 03: Added 5 tokens: 'in deutscher S', 'okeni', ' tokeni', ' wird gelernt.', 'st ein deutsch' (best summed RC=-48.5437)\n",
      "Iter 04: Added 5 tokens: 'ist ein deutsc', ' ist ein deuts', ' wird gelernt', 's are useful.', 'st ein deutsc' (best summed RC=-46.3124)\n",
      "Iter 05: Added 5 tokens: ' is learning.', ' ist ein deut', ' wird gelern', 's are useful', 'is learning.' (best summed RC=-43.0362)\n",
      "Iter 06: Added 5 tokens: ' token', ' is learning', 'Das ist ein ', ' ist ein deu', ' wird geler' (best summed RC=-40.2760)\n",
      "Iter 07: Added 5 tokens: 's are usefu', 'deutscher Sat', 'Das ist ein', ' is learnin', ' wird gele' (best summed RC=-37.6045)\n",
      "Iter 08: Added 5 tokens: 's are usef', 'Good toke', ' deutscher Sa', 'deutscher Sa', 'Das ist ei' (best summed RC=-34.2229)\n",
      "Iter 09: Added 5 tokens: 'nizers are use', 'nizers are us', 'izers are use', 'nizers are u', 'izers are us' (best summed RC=-40.9715)\n",
      "Iter 10: Added 5 tokens: ' is learni', ' wird gel', 'This toke', 'nizers are ', 'Die Token' (best summed RC=-33.3916)\n",
      "Iter 11: Added 5 tokens: 'nizer is learn', 'nizer is lear', 'izer is learn', 'zer is learn', 'nizer is lea' (best summed RC=-40.3773)\n",
      "Iter 12: Added 5 tokens: 'Good tok', 'Das ist e', 'deutscher S', ' deutscher S', 'nizer is le' (best summed RC=-31.8733)\n",
      "Iter 13: Added 5 tokens: 'This tok', 'enizers are', 'nizers are', ' wird ge', 'Die Toke' (best summed RC=-30.6743)\n",
      "Iter 14: Added 5 tokens: 'enizer is l', 'nizer is l', 'Das ist ', 'in deutscher ', 'deutscher ' (best summed RC=-29.0259)\n",
      "Iter 15: Added 5 tokens: 'enizers ar', 'nizers ar', 'Good to', ' wird g', 'Die Tok' (best summed RC=-27.8832)\n",
      "Iter 16: Added 5 tokens: 'in deutscher', 'deutscher', 'This to', ' deutscher', 'n deutscher' (best summed RC=-25.7440)\n",
      "Iter 17: Added 5 tokens: 'kenizer', 'kenize', 'enizer is ', 'isierung wird ', 'nizer is ' (best summed RC=-26.1493)\n",
      "Iter 18: Added 5 tokens: 'Das ist', 'nisierun', 'enisierun', 'deutsche', 'in deutsche' (best summed RC=-24.7115)\n",
      "Iter 19: Added 5 tokens: 'Good t', 'g wird', 'isierung wird', 'nisierung wird', 'This t' (best summed RC=-22.8927)\n",
      "Iter 20: Added 5 tokens: ' deutsch', 'in deutsch', 'Die To', 'n deutsch', 'deutsch' (best summed RC=-20.6527)\n",
      "Iter 21: Added 5 tokens: 'Das is', 'nisieru', 'enisieru', 'kenisieru', 'Good ' (best summed RC=-20.1091)\n",
      "Iter 22: Added 5 tokens: 'in deutsc', 'n deutsc', 'isierung wir', 'nisierung wir', 'ein deutsc' (best summed RC=-18.0354)\n",
      "Iter 23: Added 5 tokens: 'This ', 'Das i', 'Die T', 'Good', 'isierung wi' (best summed RC=-16.4463)\n",
      "Iter 24: Added 5 tokens: 'ein deuts', 'in deuts', 'nisier', 'enisier', 'kenisier' (best summed RC=-14.6465)\n",
      "Iter 25: Added 5 tokens: 'ein deut', ' ein deut', 'in deut', 'This', 'Das ' (best summed RC=-13.3115)\n",
      "Iter 26: Added 5 tokens: 'isierung w', 'nisierung w', 'ein deu', ' ein deu', 't ein deu' (best summed RC=-12.1501)\n",
      "Iter 27: Added 5 tokens: 'Die Tokenisie', 'nisie', 'Die ', 'enisie', 'kenisie' (best summed RC=-10.1501)\n",
      "Iter 28: Added 5 tokens: 'Goo', 'er Satz.', 'her Satz.', 'Thi', 'Das' (best summed RC=-9.4490)\n",
      "Iter 29: Added 5 tokens: 'Die Tokenisi', 's are use', ' is learn', 'nisi', 'Tokenisi' (best summed RC=-7.8318)\n",
      "Iter 30: Added 5 tokens: 'Die', 's are us', 's a', 'ein de', ' ein de' (best summed RC=-6.4149)\n",
      "\n",
      "Max iterations reached.\n",
      "\n",
      "Step 3: Enforcing vocabulary budget K=30 via Lagrangian bisection on Œª.\n",
      "  Œª=1.000000 ‚Üí used types = 11 (target 30)\n",
      "  Œª=0.500000 ‚Üí used types = 11 (target 30)\n",
      "  Œª=0.250000 ‚Üí used types = 11 (target 30)\n",
      "  Œª=0.125000 ‚Üí used types = 11 (target 30)\n",
      "  Œª=0.062500 ‚Üí used types = 11 (target 30)\n",
      "  Œª=0.031250 ‚Üí used types = 11 (target 30)\n",
      "  Œª=0.015625 ‚Üí used types = 11 (target 30)\n",
      "  Œª=0.007812 ‚Üí used types = 11 (target 30)\n",
      "Chosen Œª*=1.000000 (gap=19).\n",
      "\n",
      "Training complete. Final vocabulary size: 36\n",
      "\n",
      "--- Learned Vocabulary ---\n",
      "[' deutsch', ' is learning.', ' wird gelernt.', 'Das ist ein', 'Die ', 'Good ', 'This ', 'Tokenisierung', 'er Satz.', 's are useful.', 'tokenizer']\n",
      "['This ', 'tokenizer', ' is learning.']\n",
      "['Good ', 'tokenizer', 's are useful.']\n",
      "['Das ist ein', ' deutsch', 'er Satz.']\n",
      "['Die ', 'Tokenisierung', ' wird gelernt.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import ScalableTokenizer\n",
    "from linguistic_features import LinguisticModels\n",
    "\n",
    "# We'll use the same multilingual sample corpus.\n",
    "corpus_texts = [\n",
    "    \"This tokenizer is learning.\",\n",
    "    \"Good tokenizers are useful.\",\n",
    "    \"Das ist ein deutscher Satz.\",\n",
    "    \"Die Tokenisierung wird gelernt.\"\n",
    "]\n",
    "corpus_langs = [\"en\", \"en\", \"de\", \"de\"]\n",
    "\n",
    "# --- 1. Initialize the Tokenizer ---\n",
    "tokenizer = ScalableTokenizer(\n",
    "    max_token_len=14,\n",
    "    min_freq=1,\n",
    "    top_k_add=5, # Let's add more tokens per iteration\n",
    "    vocab_budget=30\n",
    ")\n",
    "\n",
    "# --- 2. ‚≠ê PRIME THE LINGUISTIC MODELS (The Missing Step) ---\n",
    "print(\"Priming the tokenizer with linguistic hints...\")\n",
    "\n",
    "# Create some simple \"hints\" to guide the tokenizer\n",
    "lexicon = {\"tokenizer\": 5.0, \"tokenizers\": 5.0, \"Tokenisierung\": 10.0}\n",
    "token_bigrams = {\n",
    "    (\"<BOS>\", \"InitCap\"): -0.3, # Reward sentences starting with a capital word\n",
    "    (\"lower\", \"EOS\"): -0.2,     # Reward sentences ending in a lowercase word (before punct)\n",
    "}\n",
    "\n",
    "# Now, apply these models and tune their weights\n",
    "# This is the crucial step that connects the brain to the engine.\n",
    "tokenizer.set_feature_models(\n",
    "    lexicon=lexicon,\n",
    "    token_bigram=token_bigrams,\n",
    "    mu_morph=0.25,      # Weight for morphology scores\n",
    ")\n",
    "\n",
    "# --- 3. Train the Tokenizer ---\n",
    "# Now that it has linguistic guidance, it will learn much better tokens.\n",
    "# A real scenario would use max_iterations=300 or more.\n",
    "print(\"\\nStarting the training process...\")\n",
    "tokenizer.train(corpus_texts, corpus_langs, max_iterations=30)\n",
    "\n",
    "# --- 4. Inspect the Results ---\n",
    "print(\"\\n--- Learned Vocabulary ---\")\n",
    "# Filter for multi-character words to see the good stuff\n",
    "meaningful_tokens = [tok for tok in tokenizer.vocab if len(tok) > 1]\n",
    "print(sorted(meaningful_tokens))\n",
    "for corpus_text, corpus_lang in zip(corpus_texts, corpus_langs):\n",
    "    tokens = tokenizer.tokenize(corpus_text, corpus_lang)\n",
    "    print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs197-tokenizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
